{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0c83e0",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"https://communist.red/wp-content/uploads/2017/08/Anarchist_flag.png\" width=\"300px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #bbc28d> **Sentiment Analysis in Movies IMDb** </font>\n",
    "#### <font color= #2E9AFE> `Lab 2 – Text Mining`</font>\n",
    "- <Strong> Sofía Maldonado, Diana Valdivia & Viviana Toledo </Strong>\n",
    "- <Strong> Fecha </Strong>: 20/10/2025 \n",
    "\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://communist.red/wp-content/uploads/2017/08/Anarchist_flag.png</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78feaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "# Modeling\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ba2da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "024d6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo de wikipedia\n",
    "with open(r\"text8\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "071c4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos\n",
    "# Convertir todas las palabras a minúsculas\n",
    "text = text.lower()\n",
    "\n",
    "# Quedarse solo con las palabras, lo demás lo deja como un espacio en blanco\n",
    "text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "\n",
    "# Tokenizar por el whitespace\n",
    "tokens = text.split()\n",
    "\n",
    "# Quedarnos solo con palabras con mas de una letra\n",
    "tokens = [w for w in tokens if len(w) > 1]\n",
    "\n",
    "# Quedanros solo con las primeras 50 mil\n",
    "tokens_models = tokens[:50_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eaad3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vocabulario a partir de esas palabras\n",
    "vocab = sorted(list(set(tokens_models)))\n",
    "vocab_set = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c3221fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 7979\n"
     ]
    }
   ],
   "source": [
    "# Crear los word <> index diccionarios\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "tamano = len(vocab)\n",
    "print(f\"Tamaño del vocabulario: {tamano}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d37d6155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362,\n",
       " 5086,\n",
       " 543,\n",
       " 7161,\n",
       " 4983,\n",
       " 39,\n",
       " 2868,\n",
       " 7568,\n",
       " 203,\n",
       " 2293,\n",
       " 7894,\n",
       " 1315,\n",
       " 5829,\n",
       " 3659,\n",
       " 7185,\n",
       " 2080,\n",
       " 4983,\n",
       " 7185,\n",
       " 2459,\n",
       " 6175,\n",
       " 377,\n",
       " 7185,\n",
       " 6319,\n",
       " 1792,\n",
       " 4983,\n",
       " 7185,\n",
       " 2996,\n",
       " 6175,\n",
       " 7814,\n",
       " 7185,\n",
       " 7161,\n",
       " 3870,\n",
       " 6864,\n",
       " 7568,\n",
       " 3637,\n",
       " 5258,\n",
       " 7759,\n",
       " 7280,\n",
       " 1994,\n",
       " 425,\n",
       " 96,\n",
       " 7184,\n",
       " 7568,\n",
       " 7666,\n",
       " 4466,\n",
       " 7280,\n",
       " 2014,\n",
       " 7185,\n",
       " 5077,\n",
       " 4983,\n",
       " 6673,\n",
       " 3882,\n",
       " 3312,\n",
       " 307,\n",
       " 789,\n",
       " 7089,\n",
       " 7550,\n",
       " 543,\n",
       " 5483,\n",
       " 4040,\n",
       " 1055,\n",
       " 6447,\n",
       " 1920,\n",
       " 366,\n",
       " 7185,\n",
       " 7886,\n",
       " 362,\n",
       " 3870,\n",
       " 1988,\n",
       " 3010,\n",
       " 7185,\n",
       " 3206,\n",
       " 7873,\n",
       " 490,\n",
       " 6276,\n",
       " 1255,\n",
       " 4009,\n",
       " 362,\n",
       " 543,\n",
       " 5453,\n",
       " 5342,\n",
       " 3870,\n",
       " 7185,\n",
       " 811,\n",
       " 7184,\n",
       " 6277,\n",
       " 491,\n",
       " 7529,\n",
       " 377,\n",
       " 6554,\n",
       " 772,\n",
       " 15,\n",
       " 315,\n",
       " 7212,\n",
       " 491,\n",
       " 2074,\n",
       " 3829,\n",
       " 4983,\n",
       " 7799,\n",
       " 7237,\n",
       " 4466,\n",
       " 362,\n",
       " 307,\n",
       " 5968,\n",
       " 7280,\n",
       " 6010,\n",
       " 6665,\n",
       " 4680,\n",
       " 7184,\n",
       " 166,\n",
       " 7185,\n",
       " 2378,\n",
       " 4983,\n",
       " 662,\n",
       " 3779,\n",
       " 5197,\n",
       " 7185,\n",
       " 6832,\n",
       " 7185,\n",
       " 7886,\n",
       " 369,\n",
       " 543,\n",
       " 4652,\n",
       " 366,\n",
       " 7567,\n",
       " 3882,\n",
       " 2192,\n",
       " 4897,\n",
       " 3619,\n",
       " 1220,\n",
       " 4851,\n",
       " 5054,\n",
       " 405,\n",
       " 1052,\n",
       " 5871,\n",
       " 3299,\n",
       " 417,\n",
       " 662,\n",
       " 6673,\n",
       " 3637,\n",
       " 5387,\n",
       " 4983,\n",
       " 7799,\n",
       " 491,\n",
       " 5989,\n",
       " 543,\n",
       " 662,\n",
       " 5453,\n",
       " 6920,\n",
       " 377,\n",
       " 1372,\n",
       " 2311,\n",
       " 3779,\n",
       " 366,\n",
       " 166,\n",
       " 6665,\n",
       " 6013,\n",
       " 756,\n",
       " 7554,\n",
       " 7701,\n",
       " 584,\n",
       " 4983,\n",
       " 679,\n",
       " 3705,\n",
       " 4714,\n",
       " 229,\n",
       " 377,\n",
       " 6447,\n",
       " 3165,\n",
       " 7813,\n",
       " 362,\n",
       " 3870,\n",
       " 4652,\n",
       " 2299,\n",
       " 1920,\n",
       " 1055,\n",
       " 7799,\n",
       " 3882,\n",
       " 3870,\n",
       " 203,\n",
       " 366,\n",
       " 307,\n",
       " 4988,\n",
       " 5483,\n",
       " 7678,\n",
       " 4983,\n",
       " 7799,\n",
       " 7223,\n",
       " 814,\n",
       " 7280,\n",
       " 772,\n",
       " 7408,\n",
       " 2986,\n",
       " 6673,\n",
       " 3502,\n",
       " 3550,\n",
       " 20,\n",
       " 3500,\n",
       " 351,\n",
       " 363,\n",
       " 6673,\n",
       " 4552,\n",
       " 7889,\n",
       " 7611,\n",
       " 1580,\n",
       " 2528,\n",
       " 7867,\n",
       " 6124,\n",
       " 7280,\n",
       " 2313,\n",
       " 7212,\n",
       " 3870,\n",
       " 307,\n",
       " 2108,\n",
       " 20,\n",
       " 3500,\n",
       " 2986,\n",
       " 6673,\n",
       " 4552,\n",
       " 772,\n",
       " 1007,\n",
       " 20,\n",
       " 5087,\n",
       " 377,\n",
       " 5533,\n",
       " 4037,\n",
       " 377,\n",
       " 5097,\n",
       " 497,\n",
       " 7184,\n",
       " 790,\n",
       " 5943,\n",
       " 3437,\n",
       " 3512,\n",
       " 6673,\n",
       " 7746,\n",
       " 5080,\n",
       " 5018,\n",
       " 363,\n",
       " 5608,\n",
       " 4652,\n",
       " 416,\n",
       " 2902,\n",
       " 4037,\n",
       " 377,\n",
       " 2455,\n",
       " 3637,\n",
       " 817,\n",
       " 7184,\n",
       " 3526,\n",
       " 3064,\n",
       " 729,\n",
       " 7793,\n",
       " 2346,\n",
       " 377,\n",
       " 4050,\n",
       " 2180,\n",
       " 4983,\n",
       " 4046,\n",
       " 69,\n",
       " 7767,\n",
       " 5054,\n",
       " 1895,\n",
       " 4092,\n",
       " 377,\n",
       " 3262,\n",
       " 2510,\n",
       " 52,\n",
       " 7280,\n",
       " 6123,\n",
       " 7844,\n",
       " 3145,\n",
       " 366,\n",
       " 3659,\n",
       " 7185,\n",
       " 7185,\n",
       " 369,\n",
       " 5069,\n",
       " 377,\n",
       " 6259,\n",
       " 2853,\n",
       " 363,\n",
       " 632,\n",
       " 3637,\n",
       " 7104,\n",
       " 3010,\n",
       " 376,\n",
       " 1261,\n",
       " 4037,\n",
       " 2956,\n",
       " 6594,\n",
       " 3550,\n",
       " 3637,\n",
       " 6872,\n",
       " 7968,\n",
       " 4983,\n",
       " 1298,\n",
       " 63,\n",
       " 7280,\n",
       " 4037,\n",
       " 7968,\n",
       " 6094,\n",
       " 7185,\n",
       " 5017,\n",
       " 4983,\n",
       " 7185,\n",
       " 6832,\n",
       " 3888,\n",
       " 3836,\n",
       " 377,\n",
       " 5993,\n",
       " 377,\n",
       " 5635,\n",
       " 7185,\n",
       " 6724,\n",
       " 4983,\n",
       " 7185,\n",
       " 4637,\n",
       " 4092,\n",
       " 4983,\n",
       " 7185,\n",
       " 3699,\n",
       " 7185,\n",
       " 353,\n",
       " 4983,\n",
       " 5020,\n",
       " 6621,\n",
       " 7180,\n",
       " 1190,\n",
       " 2574,\n",
       " 491,\n",
       " 6694,\n",
       " 1582,\n",
       " 7280,\n",
       " 772,\n",
       " 6033,\n",
       " 2923,\n",
       " 4983,\n",
       " 4616,\n",
       " 362,\n",
       " 841,\n",
       " 6289,\n",
       " 3637,\n",
       " 3430,\n",
       " 3437,\n",
       " 4983,\n",
       " 7795,\n",
       " 5342,\n",
       " 7920,\n",
       " 7184,\n",
       " 7185,\n",
       " 353,\n",
       " 6094,\n",
       " 283,\n",
       " 4092,\n",
       " 6603,\n",
       " 7223,\n",
       " 3362,\n",
       " 7184,\n",
       " 7185,\n",
       " 3152,\n",
       " 4367,\n",
       " 7841,\n",
       " 772,\n",
       " 3248,\n",
       " 599,\n",
       " 2587,\n",
       " 4622,\n",
       " 1055,\n",
       " 7185,\n",
       " 3454,\n",
       " 6766,\n",
       " 3010,\n",
       " 7237,\n",
       " 5552,\n",
       " 7223,\n",
       " 527,\n",
       " 599,\n",
       " 1455,\n",
       " 7185,\n",
       " 2080,\n",
       " 5054,\n",
       " 7407,\n",
       " 4159,\n",
       " 7793,\n",
       " 351,\n",
       " 2293,\n",
       " 1457,\n",
       " 4679,\n",
       " 2274,\n",
       " 7185,\n",
       " 7270,\n",
       " 4983,\n",
       " 7185,\n",
       " 2459,\n",
       " 1304,\n",
       " 7730,\n",
       " 377,\n",
       " 491,\n",
       " 1582,\n",
       " 1055,\n",
       " 6690,\n",
       " 543,\n",
       " 2923,\n",
       " 4983,\n",
       " 4616,\n",
       " 362,\n",
       " 3637,\n",
       " 7185,\n",
       " 4616,\n",
       " 2514,\n",
       " 7185,\n",
       " 2868,\n",
       " 7280,\n",
       " 7567,\n",
       " 7185,\n",
       " 7161,\n",
       " 7280,\n",
       " 4463,\n",
       " 6692,\n",
       " 5096,\n",
       " 7182,\n",
       " 1220,\n",
       " 7746,\n",
       " 4279,\n",
       " 513,\n",
       " 748,\n",
       " 1856,\n",
       " 4053,\n",
       " 3637,\n",
       " 3430,\n",
       " 4910,\n",
       " 7712,\n",
       " 1832,\n",
       " 327,\n",
       " 6217,\n",
       " 6476,\n",
       " 5020,\n",
       " 6500,\n",
       " 7970,\n",
       " 7249,\n",
       " 7804,\n",
       " 3326,\n",
       " 1995,\n",
       " 7185,\n",
       " 3694,\n",
       " 336,\n",
       " 6673,\n",
       " 7811,\n",
       " 3262,\n",
       " 4860,\n",
       " 6832,\n",
       " 4094,\n",
       " 5616,\n",
       " 5596,\n",
       " 5054,\n",
       " 5618,\n",
       " 5690,\n",
       " 543,\n",
       " 807,\n",
       " 3637,\n",
       " 369,\n",
       " 6289,\n",
       " 4466,\n",
       " 4172,\n",
       " 377,\n",
       " 4103,\n",
       " 3637,\n",
       " 7185,\n",
       " 336,\n",
       " 3685,\n",
       " 4679,\n",
       " 3312,\n",
       " 6066,\n",
       " 6833,\n",
       " 7184,\n",
       " 3326,\n",
       " 3870,\n",
       " 351,\n",
       " 363,\n",
       " 377,\n",
       " 6664,\n",
       " 491,\n",
       " 283,\n",
       " 3430,\n",
       " 373,\n",
       " 3637,\n",
       " 5020,\n",
       " 6500,\n",
       " 4855,\n",
       " 7249,\n",
       " 3637,\n",
       " 7185,\n",
       " 7224,\n",
       " 4983,\n",
       " 7185,\n",
       " 2996,\n",
       " 6175,\n",
       " 7844,\n",
       " 3145,\n",
       " 5753,\n",
       " 351,\n",
       " 2472,\n",
       " 1520,\n",
       " 5453,\n",
       " 3960,\n",
       " 315,\n",
       " 3145,\n",
       " 2059,\n",
       " 4897,\n",
       " 7567,\n",
       " 7185,\n",
       " 7886,\n",
       " 362,\n",
       " 4389,\n",
       " 4082,\n",
       " 366,\n",
       " 3316,\n",
       " 5989,\n",
       " 7237,\n",
       " 930,\n",
       " 543,\n",
       " 7185,\n",
       " 2868,\n",
       " 4350,\n",
       " 363,\n",
       " 7178,\n",
       " 377,\n",
       " 3145,\n",
       " 543,\n",
       " 7185,\n",
       " 2961,\n",
       " 4983,\n",
       " 5339,\n",
       " 362,\n",
       " 1052,\n",
       " 599,\n",
       " 7237,\n",
       " 5442,\n",
       " 4860,\n",
       " 363,\n",
       " 4679,\n",
       " 7949,\n",
       " 2638,\n",
       " 377,\n",
       " 7185,\n",
       " 7161,\n",
       " 364,\n",
       " 7746,\n",
       " 4030,\n",
       " 4344,\n",
       " 543,\n",
       " 351,\n",
       " 3784,\n",
       " 3531,\n",
       " 1055,\n",
       " 7185,\n",
       " 950,\n",
       " 3124,\n",
       " 599,\n",
       " 4641,\n",
       " 5827,\n",
       " 2373,\n",
       " 3637,\n",
       " 7185,\n",
       " 2996,\n",
       " 6175,\n",
       " 7185,\n",
       " 2868,\n",
       " 6447,\n",
       " 4042,\n",
       " 363,\n",
       " 5370,\n",
       " 3938,\n",
       " 5723,\n",
       " 3882,\n",
       " 3870,\n",
       " 1445,\n",
       " 3362,\n",
       " 7184,\n",
       " 3882,\n",
       " 7749,\n",
       " 7544,\n",
       " 5370,\n",
       " 3938,\n",
       " 5723,\n",
       " 5753,\n",
       " 7799,\n",
       " 3870,\n",
       " 5690,\n",
       " 3637,\n",
       " 5020,\n",
       " 2355,\n",
       " 2965,\n",
       " 7970,\n",
       " 7184,\n",
       " 7185,\n",
       " 7161,\n",
       " 363,\n",
       " 7746,\n",
       " 148,\n",
       " 543,\n",
       " 6447,\n",
       " 1998,\n",
       " 3882,\n",
       " 3870,\n",
       " 2914,\n",
       " 7237,\n",
       " 5910,\n",
       " 7184,\n",
       " 6690,\n",
       " 1308,\n",
       " 5723,\n",
       " 543,\n",
       " 7185,\n",
       " 2961,\n",
       " 4983,\n",
       " 4616,\n",
       " 363,\n",
       " 7208,\n",
       " 3637,\n",
       " 7799,\n",
       " 3870,\n",
       " 5690,\n",
       " 5723,\n",
       " 411,\n",
       " 7867,\n",
       " 7185,\n",
       " 2749,\n",
       " 73,\n",
       " 5690,\n",
       " 3870,\n",
       " 7191,\n",
       " 3637,\n",
       " 7237,\n",
       " 7889,\n",
       " 3326,\n",
       " 5045,\n",
       " 7185,\n",
       " 3778,\n",
       " 4983,\n",
       " 1895,\n",
       " 5690,\n",
       " 5701,\n",
       " 7804,\n",
       " 5140,\n",
       " 3316,\n",
       " 1484,\n",
       " 6213,\n",
       " 7280,\n",
       " 7567,\n",
       " 377,\n",
       " 39,\n",
       " 7192,\n",
       " 5690,\n",
       " 543,\n",
       " 7223,\n",
       " 7864,\n",
       " 6970,\n",
       " 543,\n",
       " 2674,\n",
       " 7893,\n",
       " 2914,\n",
       " 5659,\n",
       " 3637,\n",
       " 3888,\n",
       " 5387,\n",
       " 5723,\n",
       " 7002,\n",
       " 7799,\n",
       " 3326,\n",
       " 1070,\n",
       " 5488,\n",
       " 3705,\n",
       " 1088,\n",
       " 3316,\n",
       " 4199,\n",
       " 6213,\n",
       " 7280,\n",
       " 7567,\n",
       " 6123,\n",
       " 1103,\n",
       " 377,\n",
       " 3154,\n",
       " 3637,\n",
       " 61,\n",
       " 7867,\n",
       " 5608,\n",
       " 4983,\n",
       " 2511,\n",
       " 377,\n",
       " 3960,\n",
       " 5723,\n",
       " 7677,\n",
       " 4983,\n",
       " 369,\n",
       " 7811,\n",
       " 3326,\n",
       " 1070,\n",
       " 4715,\n",
       " 4717,\n",
       " 3855,\n",
       " 351,\n",
       " 2618,\n",
       " 2318,\n",
       " 7804,\n",
       " 3705,\n",
       " 377,\n",
       " 3234,\n",
       " 1707,\n",
       " 7331,\n",
       " 7185,\n",
       " 5650,\n",
       " 4983,\n",
       " 7192,\n",
       " 4044,\n",
       " 7574,\n",
       " 4044,\n",
       " 4902,\n",
       " 7811,\n",
       " 6082,\n",
       " 7185,\n",
       " 345,\n",
       " 4983,\n",
       " 7894,\n",
       " 7270,\n",
       " 3855,\n",
       " 3637,\n",
       " 5647,\n",
       " 7237,\n",
       " 7907,\n",
       " 2476,\n",
       " 7184,\n",
       " 4860,\n",
       " 5020,\n",
       " 7907,\n",
       " 5659,\n",
       " 3010,\n",
       " 7185,\n",
       " 4044,\n",
       " 4983,\n",
       " 5097,\n",
       " 7893,\n",
       " 1707,\n",
       " 2990,\n",
       " 3933,\n",
       " 7285,\n",
       " 3637,\n",
       " 1358,\n",
       " 5036,\n",
       " 7898,\n",
       " 351,\n",
       " 3812,\n",
       " 2986,\n",
       " 731,\n",
       " 7907,\n",
       " 772,\n",
       " 6493,\n",
       " 7550,\n",
       " 7280,\n",
       " 5728,\n",
       " 2589,\n",
       " 7867,\n",
       " 52,\n",
       " 7280,\n",
       " 7185,\n",
       " 4466,\n",
       " 4983,\n",
       " 5647,\n",
       " 5723,\n",
       " 3550,\n",
       " 7793,\n",
       " 3730,\n",
       " 7872,\n",
       " 2996,\n",
       " 7894,\n",
       " 1315,\n",
       " 4680,\n",
       " 377,\n",
       " 3430,\n",
       " 2904,\n",
       " 7793,\n",
       " 100,\n",
       " 3637,\n",
       " 7185,\n",
       " 6175,\n",
       " 4983,\n",
       " 5020,\n",
       " 2355,\n",
       " 2965,\n",
       " 2355,\n",
       " 3637,\n",
       " 2976,\n",
       " 5723,\n",
       " 5342,\n",
       " 4983,\n",
       " 5690,\n",
       " 3870,\n",
       " 1488,\n",
       " 3882,\n",
       " 7746,\n",
       " 2032,\n",
       " 3637,\n",
       " 4922,\n",
       " 4983,\n",
       " 7897,\n",
       " 5119,\n",
       " 3430,\n",
       " 4187,\n",
       " 377,\n",
       " 7212,\n",
       " 491,\n",
       " 2074,\n",
       " 3829,\n",
       " 4983,\n",
       " 6690,\n",
       " 4983,\n",
       " 3430,\n",
       " 3550,\n",
       " 2914,\n",
       " 4641,\n",
       " 2020,\n",
       " 2131,\n",
       " 6427,\n",
       " 3381,\n",
       " 4445,\n",
       " 6870,\n",
       " 2350,\n",
       " 3637,\n",
       " 3430,\n",
       " 7185,\n",
       " 2349,\n",
       " 377,\n",
       " 3888,\n",
       " 5138,\n",
       " 6870,\n",
       " 498,\n",
       " 7184,\n",
       " 4652,\n",
       " 1445,\n",
       " 49,\n",
       " 6665,\n",
       " 3779,\n",
       " 3659,\n",
       " 7185,\n",
       " 4907,\n",
       " 4983,\n",
       " 6832,\n",
       " 5690,\n",
       " 543,\n",
       " 6210,\n",
       " 4761,\n",
       " 6213,\n",
       " 3637,\n",
       " 3077,\n",
       " 377,\n",
       " 7185,\n",
       " 7634,\n",
       " 4907,\n",
       " 4983,\n",
       " 6673,\n",
       " 7793,\n",
       " 4510,\n",
       " 3582,\n",
       " 5054,\n",
       " 3115,\n",
       " 3637,\n",
       " 7185,\n",
       " 4573,\n",
       " 6342,\n",
       " 4983,\n",
       " 6673,\n",
       " 7184,\n",
       " 7185,\n",
       " 3705,\n",
       " 491,\n",
       " 3888,\n",
       " 5901,\n",
       " 3326,\n",
       " 167,\n",
       " 2350,\n",
       " 377,\n",
       " 2931,\n",
       " 4983,\n",
       " 344,\n",
       " 3637,\n",
       " 7811,\n",
       " 3705,\n",
       " 7907,\n",
       " 7514,\n",
       " 3637,\n",
       " 585,\n",
       " 4983,\n",
       " 2352,\n",
       " 5025,\n",
       " 7802,\n",
       " 3882,\n",
       " 7746,\n",
       " 3637,\n",
       " 7192,\n",
       " 6447,\n",
       " 3812,\n",
       " 7280,\n",
       " 2185,\n",
       " 6664,\n",
       " 2914,\n",
       " 3422,\n",
       " 5690,\n",
       " 6601,\n",
       " 1424,\n",
       " 20,\n",
       " 7254,\n",
       " 4552,\n",
       " 7819,\n",
       " 4031,\n",
       " 3500,\n",
       " 7280,\n",
       " 7088,\n",
       " 7280,\n",
       " 1912,\n",
       " 7185,\n",
       " 7225,\n",
       " 7280,\n",
       " 3422,\n",
       " 821,\n",
       " 5690,\n",
       " 377,\n",
       " 7799,\n",
       " 3316,\n",
       " 3637,\n",
       " 4719,\n",
       " 5513,\n",
       " 7184,\n",
       " 3870,\n",
       " 4719,\n",
       " 5138,\n",
       " 6664,\n",
       " 4256,\n",
       " 543,\n",
       " 569,\n",
       " 4721,\n",
       " 543,\n",
       " 3447,\n",
       " 327,\n",
       " 7185,\n",
       " 5703,\n",
       " 4983,\n",
       " 7185,\n",
       " 7225,\n",
       " 6870,\n",
       " 4822,\n",
       " 1070,\n",
       " 3423,\n",
       " 351,\n",
       " 363,\n",
       " 3326,\n",
       " 49,\n",
       " 5025,\n",
       " 7185,\n",
       " 4040,\n",
       " 2351,\n",
       " 4823,\n",
       " 3430,\n",
       " 3550,\n",
       " 7793,\n",
       " 3730,\n",
       " 5018,\n",
       " 4389,\n",
       " 3702,\n",
       " 3655,\n",
       " 366,\n",
       " 315,\n",
       " 3829,\n",
       " 4983,\n",
       " 3430,\n",
       " 7243,\n",
       " 491,\n",
       " 2173,\n",
       " 336,\n",
       " 3701,\n",
       " 362,\n",
       " 835,\n",
       " 7418,\n",
       " 3637,\n",
       " 5020,\n",
       " 2355,\n",
       " 7434,\n",
       " 2875,\n",
       " 3940,\n",
       " 7742,\n",
       " 3262,\n",
       " 5191,\n",
       " 3637,\n",
       " 1459,\n",
       " 2658,\n",
       " 3328,\n",
       " 1055,\n",
       " 6232,\n",
       " 5135,\n",
       " 1070,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_idx = [word_to_idx[w] for w in tokens_models if w in vocab_set]\n",
    "tokens_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7f8e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrar los pares para cbow\n",
    "import random\n",
    "\n",
    "def generate_cbow_pairs(tokens_idx, min_window=2, max_window=5):\n",
    "    # Guardar los pares en ubna lista\n",
    "    pairs = []\n",
    "    # Rango la longitud de el vocabulario/texto\n",
    "    n = len(tokens_idx)\n",
    "    for i in range(n):\n",
    "        # Elegir nuestro target\n",
    "        target = tokens_idx[i]\n",
    "        # Ventana random entre 2 y 5\n",
    "        window_size = random.randint(min_window, max_window)\n",
    "        # Posiciones de inicio y fin de la window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, n)\n",
    "        # Contexto de la palabra a predecir\n",
    "        context = [tokens_idx[j] for j in range(start, end) if j != i]\n",
    "        if context:\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = generate_cbow_pairs(tokens_idx, min_window=2, max_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "baeb3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: ['originated', 'as', 'term', 'of', 'abuse']  ->  Target: anarchism\n",
      "Contexto: ['anarchism', 'as', 'term', 'of', 'abuse']  ->  Target: originated\n",
      "Contexto: ['anarchism', 'originated', 'term', 'of']  ->  Target: as\n",
      "Contexto: ['anarchism', 'originated', 'as', 'of', 'abuse', 'first', 'used', 'against']  ->  Target: term\n",
      "Contexto: ['anarchism', 'originated', 'as', 'term', 'abuse', 'first', 'used', 'against']  ->  Target: of\n"
     ]
    }
   ],
   "source": [
    "# Convertir los pares CBOW a texto nuevamente\n",
    "cbow_pairs_words = [\n",
    "    ([idx_to_word[i] for i in context], idx_to_word[target])\n",
    "    for context, target in cbow_pairs\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Contexto: {cbow_pairs_words[i][0]}  ->  Target: {cbow_pairs_words[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2f02a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(tokens_idx, min_window=2, max_window=5):\n",
    "    # Guardar los pares en una lista\n",
    "    pairs = []\n",
    "    # Rango la longitud de el vocabulario/texto\n",
    "    n = len(tokens_idx)\n",
    "    for i in range(n):\n",
    "        # Elegir nuestro target\n",
    "        target = tokens_idx[i]\n",
    "        # Ventana random entre 2 y 5\n",
    "        window_size = random.randint(min_window, max_window)\n",
    "        # Posiciones de inicio y fin de la window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, n)\n",
    "        for j in range(start, end):\n",
    "            #Skipear la target\n",
    "            if j != i:\n",
    "                context = tokens_idx[j]\n",
    "                pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "skip_pairs = generate_skipgram_pairs(tokens_idx, min_window=2, max_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2768f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: anarchism  ->  Contexto: originated\n",
      "Target: anarchism  ->  Contexto: as\n",
      "Target: originated  ->  Contexto: anarchism\n",
      "Target: originated  ->  Contexto: as\n",
      "Target: originated  ->  Contexto: term\n"
     ]
    }
   ],
   "source": [
    "# Convertir los SKIP pairs a texto\n",
    "skipgram_pairs_words = [\n",
    "    (idx_to_word[target], idx_to_word[context])\n",
    "    for target, context in skip_pairs\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Target: {skipgram_pairs_words[i][0]}  ->  Contexto: {skipgram_pairs_words[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26b9a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CBOW Epoch 1/15, Loss: 433.2662\n",
      "CBOW Epoch 2/15, Loss: 411.1897\n",
      "CBOW Epoch 3/15, Loss: 382.8012\n",
      "CBOW Epoch 4/15, Loss: 352.2014\n",
      "CBOW Epoch 5/15, Loss: 332.5900\n",
      "CBOW Epoch 6/15, Loss: 322.9110\n",
      "CBOW Epoch 7/15, Loss: 316.0400\n",
      "CBOW Epoch 8/15, Loss: 310.0903\n",
      "CBOW Epoch 9/15, Loss: 304.5161\n",
      "CBOW Epoch 10/15, Loss: 299.1511\n",
      "CBOW Epoch 11/15, Loss: 293.9339\n",
      "CBOW Epoch 12/15, Loss: 288.7579\n",
      "CBOW Epoch 13/15, Loss: 283.6336\n",
      "CBOW Epoch 14/15, Loss: 278.5467\n",
      "CBOW Epoch 15/15, Loss: 273.5027\n",
      "Skip-gram Epoch 1/15, Loss: 2870.2237\n",
      "Skip-gram Epoch 2/15, Loss: 2518.0833\n",
      "Skip-gram Epoch 3/15, Loss: 2356.8852\n",
      "Skip-gram Epoch 4/15, Loss: 2269.4033\n",
      "Skip-gram Epoch 5/15, Loss: 2212.4808\n",
      "Skip-gram Epoch 6/15, Loss: 2170.1928\n",
      "Skip-gram Epoch 7/15, Loss: 2135.9753\n",
      "Skip-gram Epoch 8/15, Loss: 2106.9532\n",
      "Skip-gram Epoch 9/15, Loss: 2081.4767\n",
      "Skip-gram Epoch 10/15, Loss: 2058.7242\n",
      "Skip-gram Epoch 11/15, Loss: 2038.2626\n",
      "Skip-gram Epoch 12/15, Loss: 2019.5557\n",
      "Skip-gram Epoch 13/15, Loss: 2002.4955\n",
      "Skip-gram Epoch 14/15, Loss: 1986.8818\n",
      "Skip-gram Epoch 15/15, Loss: 1972.5140\n",
      "Vector CBOW: tensor([ 0.6095, -0.5809, -0.8809, -0.6558,  0.6214,  0.1101, -0.6542,  0.9590,\n",
      "        -0.3615, -0.6796,  1.3362,  0.9889,  0.8983,  0.0601,  0.4033,  0.4601,\n",
      "        -0.2494, -0.0819, -0.3813,  0.6250, -0.9165,  1.4120,  0.9663,  0.4452,\n",
      "        -1.2896, -0.1095, -1.4945, -0.4971, -0.8775,  1.8821, -0.6146,  1.3612,\n",
      "        -0.3109,  0.6630,  2.6413,  0.7512,  0.3856,  0.6254,  0.2010, -0.0527,\n",
      "        -1.3083, -1.1225, -0.1973,  0.3828, -1.1225,  1.2436, -0.6091,  0.6855,\n",
      "         1.6651, -0.6111, -0.8586, -0.7954,  0.1694,  1.5019,  1.2248, -0.6997,\n",
      "        -0.3580, -0.3081,  0.0794,  0.5833, -0.1114, -0.4345, -0.5660, -0.5881,\n",
      "         0.8975, -1.1000, -0.2640, -0.8379, -0.4946, -1.6202, -1.8795,  0.8040,\n",
      "        -0.8346,  0.6976,  1.2169,  1.3879, -0.8767,  0.8570, -0.1449, -0.1871,\n",
      "         0.7663, -1.3185, -0.2808, -0.6865, -1.8259,  0.8699, -1.0030, -1.0316,\n",
      "         0.3081,  0.4488, -2.5418,  1.1158, -3.0716, -0.8764,  1.1935, -0.5882,\n",
      "        -1.8190, -2.5800,  1.5633,  0.9704])\n",
      "Vector Skip-gram: tensor([-0.3843,  2.3055, -0.1456,  0.7675,  0.4467, -1.3382, -0.0689,  1.2153,\n",
      "         0.2577,  1.7607, -0.0149,  0.8110,  0.8965,  0.8539,  1.0243, -1.6861,\n",
      "        -2.1472, -0.3867,  1.0591, -1.8940,  0.4649, -1.2000, -0.3088, -1.7938,\n",
      "         0.5886,  0.6214,  0.4119,  0.0058, -1.0614,  0.8922, -0.6057,  0.1953,\n",
      "         0.0507, -1.2875,  1.7738, -0.2588,  0.5335,  0.2179,  2.2023, -0.8632,\n",
      "        -0.1860,  0.9659, -0.0598,  0.1991, -1.2768, -1.2213, -0.2792, -0.4472,\n",
      "         0.9847,  1.3120,  0.4834,  0.4706, -0.3213, -0.6020,  0.7430, -0.6290,\n",
      "         0.3567,  1.5769,  1.3155,  2.0283,  1.3212, -1.1118, -2.1600, -0.6594,\n",
      "         1.4870,  0.8511, -0.7771, -0.1155, -1.1472, -0.4452,  0.8416, -0.6221,\n",
      "         0.3215, -0.8393,  0.2103,  2.2300,  0.2801, -1.8987, -1.9509, -0.8540,\n",
      "        -0.8957, -0.6949,  2.0896, -0.9433,  0.5575, -0.8016,  1.9124,  0.8265,\n",
      "        -0.0826,  0.3572,  0.2855,  0.6135, -0.1607, -1.8610,  2.0498, -0.4234,\n",
      "         0.1230, -2.2793, -0.4831, -0.5091])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 1. Datos de ejemplo\n",
    "# tokens_idx ya está definido, vocab también\n",
    "# cbow_pairs y skip_pairs generados previamente\n",
    "# ----------------------\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ----------------------\n",
    "# 2. Definir modelos simples\n",
    "# ----------------------\n",
    "embedding_dim = 100\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, contexts):\n",
    "        # contexts: lista de tensores ya en el dispositivo correcto\n",
    "        embeds = [self.embeddings(c) for c in contexts]  # lista de [context_len, emb_dim]\n",
    "        context_embeds = torch.stack([e.mean(dim=0) for e in embeds])  # [batch_size, emb_dim]\n",
    "        out = self.output(context_embeds)\n",
    "        return out\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, centers):\n",
    "        # centers: tensor [batch_size]\n",
    "        embeds = self.embeddings(centers)  # [batch_size, emb_dim]\n",
    "        out = self.output(embeds)\n",
    "        return out\n",
    "\n",
    "# ----------------------\n",
    "# 3. Crear DataLoader con collate_fn para CBOW\n",
    "# ----------------------\n",
    "def cbow_collate(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    # Convertir contextos a tensores y mover a device\n",
    "    context_tensors = [torch.tensor(c, dtype=torch.long).to(device) for c in contexts]\n",
    "    return context_tensors, torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "# Set generator for DataLoader to avoid device conflict\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "cbow_loader = DataLoader(cbow_pairs, batch_size=1024, shuffle=True, collate_fn=cbow_collate, generator=generator)\n",
    "\n",
    "# Para Skip-gram, crear dataset en CPU y mover durante entrenamiento\n",
    "skipgram_targets = torch.tensor([t for t, c in skip_pairs], dtype=torch.long)\n",
    "skipgram_contexts = torch.tensor([c for t, c in skip_pairs], dtype=torch.long)\n",
    "skipgram_dataset = list(zip(skipgram_targets, skipgram_contexts))\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=1024, shuffle=True, generator=generator)\n",
    "\n",
    "# ----------------------\n",
    "# 4. Entrenamiento CBOW\n",
    "# ----------------------\n",
    "\n",
    "cbow_model = CBOWModel(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cbow_model.parameters(), lr=0.001)\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cbow_model.train()\n",
    "    for contexts, targets in cbow_loader:\n",
    "        # Los datos ya están en el dispositivo correcto gracias al collate_fn\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(contexts)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"CBOW Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# 5. Entrenamiento Skip-gram\n",
    "# ----------------------\n",
    "skipgram_model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(skipgram_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    skipgram_model.train()\n",
    "    for centers, contexts in skipgram_loader:\n",
    "        # Mover datos a device\n",
    "        centers = centers.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = skipgram_model(centers)\n",
    "        loss = criterion(output, contexts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Skip-gram Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ----------------------\n",
    "# 6. Obtener embeddings\n",
    "# ----------------------\n",
    "# Mover embeddings de vuelta a CPU para uso posterior\n",
    "cbow_embeddings = cbow_model.embeddings.weight.data.cpu()\n",
    "skipgram_embeddings = skipgram_model.embeddings.weight.data.cpu()\n",
    "\n",
    "# Ejemplo de vector de una palabra\n",
    "word = \"science\"\n",
    "idx = word_to_idx[word]\n",
    "print(\"Vector CBOW:\", cbow_embeddings[idx])\n",
    "print(\"Vector Skip-gram:\", skipgram_embeddings[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ee32127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Model - Top 10 Most Similar Words:\n",
      "============================================================\n",
      "\n",
      "'king':\n",
      "  ensouling: 0.3759\n",
      "  toxin: 0.3755\n",
      "  lehrman: 0.3197\n",
      "  lining: 0.3091\n",
      "  is: 0.3080\n",
      "  prices: 0.3045\n",
      "  introduced: 0.3033\n",
      "  scourge: 0.3030\n",
      "  corpus: 0.3015\n",
      "  topics: 0.3008\n",
      "\n",
      "'anarchism':\n",
      "  unencumbered: 0.4349\n",
      "  drags: 0.3969\n",
      "  argued: 0.3632\n",
      "  fought: 0.3348\n",
      "  wuxia: 0.3346\n",
      "  interest: 0.3292\n",
      "  plead: 0.3277\n",
      "  importantly: 0.3260\n",
      "  deep: 0.3223\n",
      "  magazine: 0.3222\n",
      "\n",
      "'communist':\n",
      "  dating: 0.4331\n",
      "  tombigbee: 0.3527\n",
      "  fueled: 0.3285\n",
      "  son: 0.3225\n",
      "  caught: 0.3170\n",
      "  virtuously: 0.3166\n",
      "  scalped: 0.3115\n",
      "  conventional: 0.3086\n",
      "  pictogram: 0.3026\n",
      "  soviet: 0.2981\n",
      "\n",
      "'revolution':\n",
      "  goes: 0.3917\n",
      "  escape: 0.3738\n",
      "  contributing: 0.3400\n",
      "  another: 0.3396\n",
      "  doren: 0.3245\n",
      "  ways: 0.3215\n",
      "  vocabularies: 0.3201\n",
      "  measurement: 0.3168\n",
      "  patronymic: 0.3167\n",
      "  box: 0.3164\n",
      "\n",
      "'paris':\n",
      "  group: 0.3656\n",
      "  echolalia: 0.3591\n",
      "  legislative: 0.3582\n",
      "  creating: 0.3468\n",
      "  criminal: 0.3409\n",
      "  encyclopaedia: 0.3408\n",
      "  fine: 0.3380\n",
      "  significiant: 0.3366\n",
      "  involving: 0.3295\n",
      "  who: 0.3209\n",
      "\n",
      "============================================================\n",
      "Skip-gram Model - Top 10 Most Similar Words:\n",
      "============================================================\n",
      "\n",
      "'king':\n",
      "  however: 0.4556\n",
      "  site: 0.4211\n",
      "  nation: 0.4180\n",
      "  seaport: 0.4059\n",
      "  teacher: 0.3798\n",
      "  edinburgh: 0.3726\n",
      "  tomb: 0.3623\n",
      "  strategic: 0.3535\n",
      "  atmosphere: 0.3518\n",
      "  health: 0.3489\n",
      "\n",
      "'anarchism':\n",
      "  descent: 0.3648\n",
      "  continuous: 0.3613\n",
      "  primitive: 0.3534\n",
      "  willful: 0.3506\n",
      "  scene: 0.3410\n",
      "  fusion: 0.3398\n",
      "  valhalla: 0.3357\n",
      "  immoral: 0.3353\n",
      "  socratic: 0.3200\n",
      "  righteous: 0.3177\n",
      "\n",
      "'communist':\n",
      "  berliner: 0.4301\n",
      "  cleyre: 0.3949\n",
      "  step: 0.3795\n",
      "  expenditure: 0.3530\n",
      "  challenged: 0.3512\n",
      "  accusation: 0.3440\n",
      "  bind: 0.3407\n",
      "  lecompton: 0.3383\n",
      "  biography: 0.3249\n",
      "  disobedience: 0.3244\n",
      "\n",
      "'revolution':\n",
      "  dominance: 0.4147\n",
      "  xiv: 0.4100\n",
      "  restless: 0.3992\n",
      "  artistic: 0.3839\n",
      "  subjective: 0.3735\n",
      "  agendas: 0.3668\n",
      "  partibus: 0.3660\n",
      "  internal: 0.3654\n",
      "  revenue: 0.3638\n",
      "  novelist: 0.3552\n",
      "\n",
      "'paris':\n",
      "  speaker: 0.3951\n",
      "  saint: 0.3931\n",
      "  sea: 0.3413\n",
      "  connection: 0.3279\n",
      "  aino: 0.3274\n",
      "  icd: 0.3267\n",
      "  notion: 0.3222\n",
      "  aid: 0.3185\n",
      "  prepare: 0.3160\n",
      "  selfishness: 0.3147\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Get embeddings from your trained models\n",
    "cbow_embeddings = cbow_model.embeddings.weight.data.cpu().numpy()\n",
    "skipgram_embeddings = skipgram_model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_top_similar_words(embeddings, word, top_k=10):\n",
    "    \"\"\"\n",
    "    Find top-k most similar words using cosine similarity\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the anchor word\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_embedding = embeddings[word_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity with all other words\n",
    "    similarities = cosine_similarity(word_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top-k most similar words (excluding the word itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Skip the word itself\n",
    "    \n",
    "    similar_words = []\n",
    "    for idx in similar_indices:\n",
    "        similar_words.append((idx_to_word[idx], similarities[idx]))\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Choose anchor words\n",
    "anchor_words = [\"king\", \"anarchism\", \"communist\", \"revolution\", \"paris\"]\n",
    "\n",
    "print(\"CBOW Model - Top 10 Most Similar Words:\")\n",
    "print(\"=\" * 60)\n",
    "for word in anchor_words:\n",
    "    similar_words = get_top_similar_words(cbow_embeddings, word)\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Skip-gram Model - Top 10 Most Similar Words:\")\n",
    "print(\"=\" * 60)\n",
    "for word in anchor_words:\n",
    "    similar_words = get_top_similar_words(skipgram_embeddings, word)\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
