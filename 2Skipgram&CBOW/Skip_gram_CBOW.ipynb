{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0c83e0",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"https://communist.red/wp-content/uploads/2017/08/Anarchist_flag.png\" width=\"300px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #bbc28d> **Skip-gram & CBOW Word Embeddings** </font>\n",
    "#### <font color= #2E9AFE> `Lab 2 – Text Mining`</font>\n",
    "- <Strong> Sofía Maldonado, Diana Valdivia & Viviana Toledo </Strong>\n",
    "- <Strong> Fecha </Strong>: 20/10/2025 \n",
    "\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://communist.red/wp-content/uploads/2017/08/Anarchist_flag.png</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78feaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Text Processing \n",
    "import re\n",
    "\n",
    "# Modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Visualization\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f544d1a",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Introduction** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac9847",
   "metadata": {},
   "source": [
    "Natural Language Processing encompasses all the `tasks related to making computers understand human language`, of which, word embeddings are a useful tool for achieving this goal. `Word embeddings are a numerical representation of words`, which allows computers to encode language into vectors which can be visualized in different dimensions.\n",
    "\n",
    "A common representation that word embeddings use is one-hot encoding, which vectorizes each word using binary numbers in a `vector of dimensionality 1 x corpus length`, for example:  \n",
    "\n",
    "<div style=\"background-color: white; text-align: center; padding: 1em; margin-bottom: 0.5em;\">\n",
    "<img src=\"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-40dd0ac8f7ba6930347fc88ac01ef5b8_l3.svg\" style=\"display: inline-block;\"/>\n",
    "\n",
    "<div style=\"background-color: white; text-align: center; padding: 0.5em;\">\n",
    "    <img src=\"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-aca3b72bad8941a430b71c9946bf01b3_l3.svg\" style=\"display: inline-block;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1baed6",
   "metadata": {},
   "source": [
    "However, such encoding can quickly scale up with large vocabularies, resulting in a curse of dimensionality. Additionally, the embeddings are susceptible to changes in the corpora size, and the vectors dont encapsulate word meaning. The one-hot encoding 'indexes' words, but is unable to capture semantic and syntantic information; the values in the `vectors must somehow quantify the meaning of the words they represent`.\n",
    "\n",
    "To solve this problem, Word2Vec was introduced, a technique which generates `embeddings based on word similarity`, allowing them to be close to each other in a vectorized space in terms of cosine distance. There are two main algorithms to obtain a Word2Vec implementation: Continuous Bag of Words (CBOW) and Skip-Gram, which make use of neural network models.\n",
    "\n",
    "In this notebook, we will explore both models and implement them on a corpus about Anarchy on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f943278",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Preprocessing** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024d6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Wikipedia file \n",
    "with open(r\"text8\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cf258",
   "metadata": {},
   "source": [
    "The preprocessing task involves:\n",
    "\n",
    "- Normalization of Corpus (ensuring words are in lowercase)\n",
    "\n",
    "- Word extraction\n",
    "\n",
    "- Tokenization by whitespace\n",
    "\n",
    "- Remove single-letter tokens\n",
    "\n",
    "- Select a corpus of 50,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071c4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "# Convert every word to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Keep only the words, leave the rest as whitespace\n",
    "text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "\n",
    "# Tokenize by whitespace\n",
    "tokens = text.split()\n",
    "\n",
    "# Keep only more than single letter words\n",
    "tokens = [w for w in tokens if len(w) > 1]\n",
    "\n",
    "# Keep first 50k words\n",
    "tokens_models = tokens[:50_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a3dea",
   "metadata": {},
   "source": [
    "Next, we create a vocabulary from those words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaad3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary based on the clean corpora\n",
    "vocab = sorted(list(set(tokens_models)))\n",
    "vocab_set = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf8544",
   "metadata": {},
   "source": [
    "After creating a vocabulary, we build up a dictionary of word-index pairs for the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3221fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7979\n"
     ]
    }
   ],
   "source": [
    "# Create word-index dictionaries\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "length = len(vocab)\n",
    "print(f\"Vocabulary Size: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239385d9",
   "metadata": {},
   "source": [
    "And finally, we can visualize the id of the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d37d6155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362, 5086, 543, 7161, 4983, 39, 2868, 7568, 203, 2293]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_idx = [word_to_idx[w] for w in tokens_models if w in vocab_set]\n",
    "tokens_idx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2503c15",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Modeling** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ddd58",
   "metadata": {},
   "source": [
    "CBOW and Skip-gram are algorithms that work on word pairs, meaning, `for labeling word similarity, words are paired together and then analyzed`. The pairing is done by a window-size, which selects words behind and after the target, and pairs them together. Thus, this will be the first step in our respective modelings.\n",
    "\n",
    "Both models are going to be implemented using Pytorch, since they are, essentially, neural networks. Taking advantage of this, we will first check if cuda is available. If it is, we're going to run our models in the computer's gpu, as it speeds up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa081e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8186ab0",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **1. Skip-Gram** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523202b3",
   "metadata": {},
   "source": [
    "The Skip-Gram algorithm tries to extract the semantics of words (context) by `predicting the context words using the main word`. By picking the word pais of the target word, each of them is run through a neural network model with one hidden layer:\n",
    "\n",
    "<p align= \"center\">\n",
    "    <img src='https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Baeldung-word-embeddings-1-656x1024-1.png' width=\"420px\" height=\"560px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e4f99",
   "metadata": {},
   "source": [
    "Firstly, we need to generate the skipgram pairs. We're using a word window of 2 to 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca5e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(tokens_idx, min_window=2, max_window=5):\n",
    "    # Guardar los pares en una lista\n",
    "    pairs = []\n",
    "    # Rango la longitud de el vocabulario/texto\n",
    "    n = len(tokens_idx)\n",
    "    for i in range(n):\n",
    "        # Elegir nuestro target\n",
    "        target = tokens_idx[i]\n",
    "        # Ventana random entre 2 y 5\n",
    "        window_size = random.randint(min_window, max_window)\n",
    "        # Posiciones de inicio y fin de la window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, n)\n",
    "        for j in range(start, end):\n",
    "            #Skipear la target\n",
    "            if j != i:\n",
    "                context = tokens_idx[j]\n",
    "                pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "skip_pairs = generate_skipgram_pairs(tokens_idx, min_window=2, max_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23607b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: anarchism  -->  Context: originated\n",
      "Target: anarchism  -->  Context: as\n",
      "Target: anarchism  -->  Context: term\n",
      "Target: anarchism  -->  Context: of\n",
      "Target: originated  -->  Context: anarchism\n"
     ]
    }
   ],
   "source": [
    "# Convert Skip-Gram pairs to text\n",
    "skipgram_pairs_words = [\n",
    "    (idx_to_word[target], idx_to_word[context])\n",
    "    for target, context in skip_pairs\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Target: {skipgram_pairs_words[i][0]}  -->  Context: {skipgram_pairs_words[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120a558",
   "metadata": {},
   "source": [
    "After pairing the words, we can proceed to model the data. We're using 15 epochs, a CrossEntropyLoss criterion, and an Adam optimizer with a learning rate of 0.001: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713cfb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Epoch 1/15, Loss: 2872.5853\n",
      "Skip-gram Epoch 2/15, Loss: 2519.8706\n",
      "Skip-gram Epoch 3/15, Loss: 2359.8234\n",
      "Skip-gram Epoch 4/15, Loss: 2271.6430\n",
      "Skip-gram Epoch 5/15, Loss: 2214.5140\n",
      "Skip-gram Epoch 6/15, Loss: 2171.9992\n",
      "Skip-gram Epoch 7/15, Loss: 2137.4398\n",
      "Skip-gram Epoch 8/15, Loss: 2108.2206\n",
      "Skip-gram Epoch 9/15, Loss: 2082.5474\n",
      "Skip-gram Epoch 10/15, Loss: 2059.6493\n",
      "Skip-gram Epoch 11/15, Loss: 2039.0587\n",
      "Skip-gram Epoch 12/15, Loss: 2020.4535\n",
      "Skip-gram Epoch 13/15, Loss: 2003.3486\n",
      "Skip-gram Epoch 14/15, Loss: 1987.8195\n",
      "Skip-gram Epoch 15/15, Loss: 1973.5437\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "embedding_dim = 100\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# ==========================\n",
    "# Model Definition\n",
    "# ==========================\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, centers):\n",
    "        # centers: tensor [batch_size]\n",
    "        embeds = self.embeddings(centers)  # [batch_size, emb_dim]\n",
    "        out = self.output(embeds)\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# Data Loader\n",
    "# ==========================  \n",
    "generator = torch.Generator(device=device)              # Set generator\n",
    "# Create dataset in CPU \n",
    "skipgram_targets = torch.tensor([t for t, c in skip_pairs], dtype=torch.long)\n",
    "skipgram_contexts = torch.tensor([c for t, c in skip_pairs], dtype=torch.long)\n",
    "# Combine the targets and context into a list pair for training\n",
    "skipgram_dataset = list(zip(skipgram_targets, skipgram_contexts))\n",
    "\n",
    "# Data Loader for batch processing\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=1024, shuffle=True, generator=generator)\n",
    "\n",
    "# ==========================\n",
    "# Model Training \n",
    "# ==========================\n",
    "skipgram_model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(skipgram_model.parameters(), lr=0.001)\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    skipgram_model.train()\n",
    "    for centers, contexts in skipgram_loader:\n",
    "        # Mover datos a device\n",
    "        centers = centers.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = skipgram_model(centers)\n",
    "        loss = criterion(output, contexts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Skip-gram Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038f819",
   "metadata": {},
   "source": [
    "Finally, we can obtain the embeddings produced by Skip-Gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3b6180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Vector of 'science': \n",
      " tensor([ 2.1129,  0.0386, -1.3943,  0.9383, -2.0695,  0.8647,  0.5960, -0.1876,\n",
      "        -0.4468, -2.4660, -0.0727,  1.1486, -1.0809,  1.5564, -1.8450,  0.5167,\n",
      "         0.4109,  0.3302, -0.0590,  0.9703, -1.1504,  2.1644,  0.8036, -0.1588,\n",
      "        -1.0557,  2.0893, -1.1484, -1.3856, -0.3413, -0.8697,  1.4587, -1.6520,\n",
      "         1.8534, -0.0451, -0.5159, -2.7897, -0.5317,  0.5058, -0.1839, -1.7234,\n",
      "         1.6467, -1.4718, -0.9714,  0.0431,  0.2521,  0.4107, -1.0097,  1.4442,\n",
      "        -0.0497,  0.3665,  0.0576, -0.4305,  0.4509, -0.1748,  0.2137,  0.9769,\n",
      "         0.0594, -0.0921,  0.5405, -0.1075, -0.1902, -0.3547,  0.8332, -0.7606,\n",
      "         0.2905, -2.3213, -0.8157, -0.6477, -1.2070,  0.2294, -0.4838,  1.0091,\n",
      "        -1.1870,  1.3637,  0.4349,  0.3887,  0.5620, -0.5153,  0.8767,  1.3278,\n",
      "         0.0188, -0.4318,  1.4938,  3.1381,  0.9087,  1.5092, -2.1301,  0.6259,\n",
      "        -1.3380, -0.8744, -1.6318, -0.7296,  1.1615, -0.3448,  2.0887, -1.9452,\n",
      "         0.8824, -2.3936, -0.3227, -0.4313])\n"
     ]
    }
   ],
   "source": [
    "# Move embeddings to CPU for further usage (Visualization)\n",
    "skipgram_embeddings = skipgram_model.embeddings.weight.data.cpu()\n",
    "\n",
    "# Visualize the vector of a word\n",
    "word = \"science\"\n",
    "idx = word_to_idx[word]\n",
    "print(f\"Skip-Gram Vector of '{word}': \\n {skipgram_embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4423d51",
   "metadata": {},
   "source": [
    "For a clearer understanding of the embeddings, let's take a look at the 10 most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5482d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model - Top 10 Most Similar Words related to ['king', 'anarchism', 'communist', 'revolution', 'paris']:\n",
      "========================================================================================================================\n",
      "\n",
      "'king':\n",
      "  leadership: 0.4042\n",
      "  organon: 0.3807\n",
      "  shrugged: 0.3530\n",
      "  jewish: 0.3394\n",
      "  rapidly: 0.3354\n",
      "  extra: 0.3313\n",
      "  bmc: 0.3242\n",
      "  rest: 0.3216\n",
      "  nomadic: 0.3205\n",
      "  omnipotence: 0.3202\n",
      "\n",
      "'anarchism':\n",
      "  insurrectionary: 0.4362\n",
      "  instrumental: 0.4109\n",
      "  descendants: 0.3905\n",
      "  argued: 0.3748\n",
      "  nationalism: 0.3701\n",
      "  capitalists: 0.3612\n",
      "  ensure: 0.3539\n",
      "  wording: 0.3377\n",
      "  tit: 0.3329\n",
      "  seize: 0.3260\n",
      "\n",
      "'communist':\n",
      "  presupposes: 0.3517\n",
      "  version: 0.3488\n",
      "  judge: 0.3369\n",
      "  philosophies: 0.3295\n",
      "  associating: 0.3245\n",
      "  merging: 0.3232\n",
      "  slow: 0.3223\n",
      "  litt: 0.3200\n",
      "  saying: 0.3185\n",
      "  telling: 0.3146\n",
      "\n",
      "'revolution':\n",
      "  create: 0.4140\n",
      "  seymour: 0.4024\n",
      "  saving: 0.3798\n",
      "  characterized: 0.3624\n",
      "  chomsky: 0.3585\n",
      "  catechism: 0.3463\n",
      "  fowlers: 0.3453\n",
      "  desires: 0.3452\n",
      "  quality: 0.3446\n",
      "  ct: 0.3390\n",
      "\n",
      "'paris':\n",
      "  homefront: 0.4380\n",
      "  vague: 0.4045\n",
      "  community: 0.3612\n",
      "  definition: 0.3508\n",
      "  promotional: 0.3362\n",
      "  observes: 0.3341\n",
      "  novelists: 0.3339\n",
      "  waterway: 0.3317\n",
      "  philosophers: 0.3302\n",
      "  founded: 0.3296\n"
     ]
    }
   ],
   "source": [
    "def get_top_similar_words(embeddings, word, top_k=10):\n",
    "    \"\"\"\n",
    "    Find top-k most similar words using cosine similarity\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the anchor word\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_embedding = embeddings[word_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity with all other words\n",
    "    similarities = cosine_similarity(word_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top-k most similar words (excluding the word itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Skip the word itself\n",
    "    \n",
    "    similar_words = []\n",
    "    for idx in similar_indices:\n",
    "        similar_words.append((idx_to_word[idx], similarities[idx]))\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Anchor words\n",
    "anchor_words = [\"king\", \"anarchism\", \"communist\", \"revolution\", \"paris\"] \n",
    "\n",
    "print(f\"Skip-gram Model - Top 10 Most Similar Words related to {anchor_words}:\")\n",
    "print(\"=\" * 120)\n",
    "for word in anchor_words:\n",
    "    similar_words = get_top_similar_words(skipgram_embeddings.numpy(), word)\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172126e",
   "metadata": {},
   "source": [
    "There are some words that are not exactly related to the anchor word, however, they may appear in similar contexts. **Skip-Gram works best with small datasets**, and since we're working with a large corpora (50,000 words) it may have some trouble identifying context. The most common associated words with our anchor words are:\n",
    "\n",
    "- **King** <--> Battles, Hawthorne\n",
    "- **Anarchism** <--> Zinovievna, Leftism\n",
    "- **Communist** <--> Troilus, Revolutionary\n",
    "- **Revolution** <--> Hoosier, Ukraine\n",
    "- **Paris** <--> York, Hart\n",
    "\n",
    "From this, we can see some disonances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec4d13",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **2. CBOW** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54e6f5",
   "metadata": {},
   "source": [
    "CBOW algorithm works similar to Skip-Gram, but it does the reverse operation, the model tries to `predict the main word using the context words`. Therefore, the neural network is a mirror of Skip-gram:\n",
    "\n",
    "<p align= \"center\">\n",
    "    <img src='https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1-768x462.png' width=\"480px\" height=\"360px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f801fa",
   "metadata": {},
   "source": [
    "Same as with Skip-Gram, we start by generating word pairs, with a window of 2 to 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f8e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar los pares para cbow\n",
    "def generate_cbow_pairs(tokens_idx, min_window=2, max_window=5):\n",
    "    # Guardar los pares en una lista\n",
    "    pairs = []\n",
    "    # Rango la longitud de el vocabulario/texto\n",
    "    n = len(tokens_idx)\n",
    "    for i in range(n):\n",
    "        # Elegir nuestro target\n",
    "        target = tokens_idx[i]\n",
    "        # Ventana random entre 2 y 5\n",
    "        window_size = random.randint(min_window, max_window)\n",
    "        # Posiciones de inicio y fin de la window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, n)\n",
    "        # Contexto de la palabra a predecir\n",
    "        context = [tokens_idx[j] for j in range(start, end) if j != i]\n",
    "        if context:\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "cbow_pairs = generate_cbow_pairs(tokens_idx, min_window=2, max_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baeb3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['originated', 'as', 'term', 'of', 'abuse']                              -->               Target: anarchism\n",
      "Context: ['anarchism', 'as', 'term', 'of', 'abuse', 'first']                      -->              Target: originated\n",
      "Context: ['anarchism', 'originated', 'term', 'of', 'abuse', 'first']              -->                      Target: as\n",
      "Context: ['anarchism', 'originated', 'as', 'of', 'abuse', 'first', 'used']        -->                    Target: term\n",
      "Context: ['as', 'term', 'abuse', 'first']                                         -->                      Target: of\n"
     ]
    }
   ],
   "source": [
    "# Convert CBOW pairs to text\n",
    "cbow_pairs_words = [\n",
    "    ([idx_to_word[i] for i in context], idx_to_word[target])\n",
    "    for context, target in cbow_pairs\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    context_str = f'Context: {cbow_pairs_words[i][0]}'\n",
    "    target_str = f'Target: {cbow_pairs_words[i][1]}'\n",
    "    print(f\"{context_str.ljust(80)}  -->  {target_str.rjust(30)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1662121",
   "metadata": {},
   "source": [
    "After pairing the words, we can proceed to model the data. We're using 15 epochs, a CrossEntropyLoss criterion, and an Adam optimizer with a learning rate of 0.001: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8f45893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Epoch 1/15, Loss: 433.7627\n",
      "CBOW Epoch 2/15, Loss: 411.2242\n",
      "CBOW Epoch 3/15, Loss: 382.6940\n",
      "CBOW Epoch 4/15, Loss: 352.6040\n",
      "CBOW Epoch 5/15, Loss: 333.5147\n",
      "CBOW Epoch 6/15, Loss: 323.9343\n",
      "CBOW Epoch 7/15, Loss: 317.0733\n",
      "CBOW Epoch 8/15, Loss: 311.0076\n",
      "CBOW Epoch 9/15, Loss: 305.3666\n",
      "CBOW Epoch 10/15, Loss: 299.8972\n",
      "CBOW Epoch 11/15, Loss: 294.5601\n",
      "CBOW Epoch 12/15, Loss: 289.3416\n",
      "CBOW Epoch 13/15, Loss: 284.1457\n",
      "CBOW Epoch 14/15, Loss: 279.0069\n",
      "CBOW Epoch 15/15, Loss: 273.8974\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Model Definition\n",
    "# ==========================\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, contexts):\n",
    "        # contexts: lista de tensores ya en el dispositivo correcto\n",
    "        embeds = [self.embeddings(c) for c in contexts]  # lista de [context_len, emb_dim]\n",
    "        context_embeds = torch.stack([e.mean(dim=0) for e in embeds])  # [batch_size, emb_dim]\n",
    "        out = self.output(context_embeds)\n",
    "        return out\n",
    "    \n",
    "# ==========================\n",
    "# DataLoader \n",
    "# ==========================\n",
    "def cbow_collate(batch):\n",
    "    contexts, targets = zip(*batch)\n",
    "    # Convertir contextos a tensores y mover a device\n",
    "    context_tensors = [torch.tensor(c, dtype=torch.long).to(device) for c in contexts]\n",
    "    return context_tensors, torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "cbow_loader = DataLoader(cbow_pairs, batch_size=1024, shuffle=True, collate_fn=cbow_collate, generator=generator)\n",
    "\n",
    "# ==========================\n",
    "# Model Training \n",
    "# ==========================\n",
    "cbow_model = CBOWModel(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cbow_model.parameters(), lr=0.001)\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cbow_model.train()\n",
    "    for contexts, targets in cbow_loader:\n",
    "        # Los datos ya están en el dispositivo correcto gracias al collate_fn\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(contexts)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"CBOW Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f08367",
   "metadata": {},
   "source": [
    "After our CBOW model has been trained, it's time to take a look at the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2522862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Vector of 'science': \n",
      " tensor([ 1.2161, -1.0920, -0.6482, -1.7903,  0.6046,  1.5569,  2.2029, -0.9556,\n",
      "         0.5691, -0.3546, -1.7698,  0.7636,  1.0751,  1.1890,  2.5660,  0.2730,\n",
      "        -0.2542, -2.3767,  0.9937,  0.1385, -3.2756,  0.6779,  0.7277,  0.7702,\n",
      "        -0.3677, -2.7889,  1.1537,  0.4579,  2.5921, -0.4360, -1.7802,  0.3261,\n",
      "         0.0167,  1.1755, -2.8044,  1.1860,  0.9546, -1.1252, -0.5227, -0.7571,\n",
      "         1.2803, -0.1826,  0.2124, -0.9714, -0.3592,  0.8604,  0.9870,  0.5763,\n",
      "         0.1255,  0.0471,  0.7244,  1.9093, -0.8323, -0.5541, -0.4015, -0.7600,\n",
      "         0.7204,  0.6383, -1.0407,  0.3809,  2.2629,  0.1622, -0.9786,  0.5457,\n",
      "        -0.9005, -0.3926,  0.2537,  0.6134,  0.3777, -1.0227,  1.4897,  0.2532,\n",
      "         0.2899, -0.4815, -3.6886, -0.8326,  2.2021,  1.3784, -0.4389,  0.4750,\n",
      "         0.1050,  1.0012,  1.9981,  0.5731, -1.3327, -1.2035, -0.7176,  0.2909,\n",
      "        -1.6777, -0.2517,  1.2730,  0.6230, -0.7558,  0.4057, -1.0297, -0.8882,\n",
      "        -1.4737, -2.3611,  1.9109,  1.2953])\n"
     ]
    }
   ],
   "source": [
    "# Move embeddings to CPU for further usage (Visualization)\n",
    "cbow_embeddings = cbow_model.embeddings.weight.data.cpu()\n",
    "\n",
    "# Visualize the vector of a word\n",
    "word = \"science\"\n",
    "idx = word_to_idx[word]\n",
    "print(f\"CBOW Vector of '{word}': \\n {cbow_embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd3548",
   "metadata": {},
   "source": [
    "And, a view of the top 10 most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ee32127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Model - Top 10 Most Similar Words related to ['king', 'anarchism', 'communist', 'revolution', 'paris']:\n",
      "========================================================================================================================\n",
      "\n",
      "'king':\n",
      "  newspaperman: 0.3928\n",
      "  deed: 0.3694\n",
      "  borrowed: 0.3670\n",
      "  cellular: 0.3636\n",
      "  kneels: 0.3533\n",
      "  preserved: 0.3525\n",
      "  delivered: 0.3523\n",
      "  act: 0.3355\n",
      "  armor: 0.3275\n",
      "  coin: 0.3237\n",
      "\n",
      "'anarchism':\n",
      "  crafoord: 0.3888\n",
      "  unchanging: 0.3637\n",
      "  formation: 0.3611\n",
      "  daughter: 0.3486\n",
      "  cdd: 0.3377\n",
      "  kira: 0.3355\n",
      "  giving: 0.3355\n",
      "  more: 0.3330\n",
      "  twentieth: 0.3218\n",
      "  develop: 0.3207\n",
      "\n",
      "'communist':\n",
      "  worshipped: 0.3620\n",
      "  spontaneity: 0.3332\n",
      "  periodically: 0.3144\n",
      "  cite: 0.3139\n",
      "  binary: 0.3104\n",
      "  del: 0.3078\n",
      "  attributive: 0.3069\n",
      "  dark: 0.3067\n",
      "  definition: 0.3043\n",
      "  evidenced: 0.3036\n",
      "\n",
      "'revolution':\n",
      "  ibycus: 0.4304\n",
      "  new: 0.3608\n",
      "  ruthless: 0.3538\n",
      "  rescind: 0.3521\n",
      "  indivisible: 0.3512\n",
      "  pdd: 0.3363\n",
      "  heals: 0.3355\n",
      "  reflected: 0.3230\n",
      "  pinyin: 0.3224\n",
      "  chairman: 0.3185\n",
      "\n",
      "'paris':\n",
      "  rail: 0.4085\n",
      "  knew: 0.3980\n",
      "  ten: 0.3634\n",
      "  estimated: 0.3606\n",
      "  hugo: 0.3585\n",
      "  categories: 0.3312\n",
      "  deduction: 0.3300\n",
      "  event: 0.3292\n",
      "  aide: 0.3223\n",
      "  slaves: 0.3181\n"
     ]
    }
   ],
   "source": [
    "print(f\"CBOW Model - Top 10 Most Similar Words related to {anchor_words}:\")\n",
    "print(\"=\" * 120)\n",
    "for word in anchor_words:\n",
    "    similar_words = get_top_similar_words(cbow_embeddings.numpy(), word)\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e479d",
   "metadata": {},
   "source": [
    "CBOW seems to perform better when it comes to identifying word context, since it doesn't have the Skip-Gram limitations for vocabulary size. The most common associated words with our anchor words are:\n",
    "\n",
    "- **King** <--> Declaration\n",
    "- **Anarchism** <--> Falls\n",
    "- **Communist** <--> Provided, Nourishing\n",
    "- **Revolution** <--> Call, Manifested\n",
    "- **Paris** <--> Group, Realm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42636494",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Embeddings Visualization** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3a42b",
   "metadata": {},
   "source": [
    "The similarity between words can also be observed graphically. However, due to the high dimensionality of the embeddings, dimensionality reduction techniques need to be applied first. For this project, two techniques will be tested: `t-SNE` and `UMAP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b001b",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **1. t-SNE** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece07542",
   "metadata": {},
   "source": [
    "`t-SNE` (or T-distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique developed by Laurens van der Maaten and Geoffrey Hinton in 2008.\n",
    "\n",
    "It works in two steps. First, it creates a probability distribution on pairs of high-dimension objects in which similar objects have higher probability, with dissimilar objects having lower probability. At the same time, it creates another probability distribution with the points in the lower-dimension map. \n",
    "\n",
    "Then, it minimizes the [KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions. This algorithm uses Euclidean distance as the base of its similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbfee8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting 200 random words to visualize\n",
    "tsne_samples = 200\n",
    "\n",
    "np.random.seed(42) # Seed so that re-runs don't affect the output\n",
    "\n",
    "# Model subsets\n",
    "idx_2 = np.random.choice(len(vocab), tsne_samples, replace=False)\n",
    "cbow_subset = cbow_embeddings[idx_2]\n",
    "skipgram_subset = skipgram_embeddings[idx_2]\n",
    "\n",
    "# Vocabulary subset for comparison\n",
    "vocab_subset = [vocab[i] for i in idx_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb4de5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": "blue",
          "size": 6
         },
         "mode": "markers+text",
         "showlegend": false,
         "text": [
          "eugene",
          "go",
          "telos",
          "insurrectionary",
          "welfare",
          "interviews",
          "dutch",
          "greenspan",
          "mentally",
          "larmes",
          "although",
          "camel",
          "iade",
          "showed",
          "depended",
          "overhead",
          "surmounted",
          "shyness",
          "reel",
          "promotional",
          "pretending",
          "intentionally",
          "surrender",
          "hector",
          "sexuality",
          "visions",
          "cbs",
          "deficit",
          "new",
          "loday",
          "weapon",
          "morte",
          "spite",
          "identifies",
          "organising",
          "dilorenzo",
          "burney",
          "archaic",
          "used",
          "profit",
          "engineer",
          "rainbow",
          "noticing",
          "clear",
          "rivers",
          "campaigning",
          "altruisme",
          "les",
          "exceeding",
          "tackle",
          "tzetzes",
          "broken",
          "games",
          "odyssey",
          "major",
          "virtuously",
          "introduction",
          "ramifications",
          "limp",
          "school",
          "harry",
          "initiate",
          "cheated",
          "approved",
          "parliamentary",
          "performances",
          "pool",
          "prefers",
          "discharge",
          "residents",
          "homer",
          "kneels",
          "county",
          "dares",
          "com",
          "shares",
          "climatic",
          "experiences",
          "disguises",
          "ideologies",
          "storytellers",
          "ibycus",
          "retardation",
          "catechism",
          "heatstroke",
          "facets",
          "tiepolo",
          "categoriae",
          "valued",
          "economical",
          "net",
          "manipulation",
          "kidnap",
          "calm",
          "beach",
          "respiratione",
          "wade",
          "writers",
          "spooner",
          "bill",
          "farm",
          "oases",
          "behaved",
          "road",
          "cellular",
          "barbershop",
          "espouses",
          "eastern",
          "un",
          "question",
          "receive",
          "means",
          "interested",
          "marxism",
          "righteous",
          "seemed",
          "seward",
          "prosperous",
          "creating",
          "divine",
          "misdirection",
          "exposed",
          "disorders",
          "amendment",
          "office",
          "feminist",
          "unrealistic",
          "william",
          "icd",
          "epicureans",
          "buddhism",
          "worship",
          "reasoned",
          "prepare",
          "expansion",
          "leventhal",
          "fowlers",
          "nationalists",
          "commercial",
          "computed",
          "participating",
          "emancipation",
          "rutgers",
          "understand",
          "das",
          "appomattox",
          "assist",
          "knows",
          "kelly",
          "healing",
          "kearns",
          "primitivism",
          "class",
          "arthur",
          "terrific",
          "http",
          "abroad",
          "multiple",
          "wake",
          "largest",
          "slanderers",
          "panicked",
          "ranges",
          "soil",
          "nursery",
          "servitude",
          "accounted",
          "back",
          "talk",
          "tremendous",
          "mid",
          "filter",
          "irving",
          "study",
          "applications",
          "planets",
          "tijuana",
          "divisions",
          "lix",
          "eclectic",
          "lash",
          "comprise",
          "wearing",
          "investigated",
          "washington",
          "principle",
          "colonies",
          "provisional",
          "films",
          "apart",
          "refuge",
          "roots",
          "flag",
          "caelo",
          "military",
          "energies",
          "ascended",
          "warms",
          "states",
          "dans"
         ],
         "textfont": {
          "size": 9
         },
         "textposition": "top center",
         "type": "scatter",
         "x": {
          "bdata": "WOByPs84gL8A+qK/TvlaPxUGOsDSW40+I85wPij7VL12/7a+tGstwI86jj/Om4W//DU8QMTxA0Awi5w8GJhNwHL4PECn3io/bFPvv/gBIEDCYghAhavnvVkmkz6y8QtAVKv+vhK8tr9YILw/nQF2v8ubkLr+h5O9hjeDv2JwR8AtlcM/meKavzJKmT2JkZI+geLhPeircMBW0Pu/vwzEPqSF1z5wKSu/MDCXv7UXND9/4yVAA0q5PjL0CsBM9yrAXL2lPy6W2r/6JOq/FgETwDjyK8DxzFs/GCijP1XbC0D3pfe/1higvwr/Pr+JEpC+g/zKv3oevD/0Jmm/sv5MwGEZOL/ZGUJAvxmsv8TuCr9xcFy/Z4X2v3nZwD9AeaY/hm7Fv+X/S8BYvyHAeaoSwCXZlL/5ZHbAwv7WvyRiij8ZKSzA0jPAPuARZ79zyhq/9G1WPzORsz89bgBAdUWFv/YncsDAN4E+Ro1AP6JekD2O9o6/MYMBQOZ8Sj8L0gs/blzrvviEpz/L/JM/hAKQvzKI9r+IC0I/YC1FwL93Fr6S87g+4cQywMbumj9zHoe+Z2J6vs4ZsT/agam+UXzrvZNvXr5kRrm/3pAnPxMnOMBVnSJA8SSFv0j1Vr1W5n6/XD/VvyVCSL00qqC/js4mwASqtr6Qeew9k72CP3KUvz3Allc/jTlyvTkYzr5q09i/dpB7P0gJ0b8YjfM/HlLnPiguer/O+AO/GFvdv/qC5j53Dbw+jDjev1uyuT9Vb6G/RUAvv9tKhT3eUmS+X5b1v2H1Rr/r4Zs/fLbgP14/Fb8WY527b0DEP8xT579i2R4/aidTv0gM0L/ScSw/nX0gwILseb9WVwhACVY8QKOKsr+HMBfAWuXYv++qpjyT2GE/nVaUP7fMUj/0WJA/gZ0JQLDFK78KuwNA6kQUQAmYlL/EJZq/k2WJvqpWhz7DBK0/1EyCv3KvIcBUzui/gzR3wJsuK8C0dlXAlQguwOk8Ar/GU/c/WmOsvw6o4D63CMq+fU4KPlggab9l2ti/zbDkuzpav75V90PACmkVwGkfBMA=",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "hTR8P7jdIMAmIrg/2Zw7P1Fy2b8Y+Mm/BGYFvjW5VT4jlqg+nCkmwJ29aEDekoG/shY0P4ByEr02Vr8/cc/Tv7yNHkD0Gs0/uVfLvyo8Vr5wleW9IG6avtJnkr/YqrY/p9ywPx5WAz8j/9i/IsliPxlF9r8bfA3AB3b5PwtIOr6HXdA+f0S0PsaPWz9nlaG/IWqwvMVoC763XiNAEQ4WQJF0QT/akrk//H5VvwMWC0DUDKi//cG7v+l2279HUyTAc0dlQJEEwL8SHMA/thMoP3CyBkDJ6Ru+ya8rPvcVFz96enG/4VFiP6CZ3L0XSKE/9o6Rvp9Pmz9RBEVAccJrv638OUBRlh5AI8wHv4JM3D++5d0+2dbYP6KGCT9Jdi/Ag85Sv9r37D9rwlBASPaEP/lEDEBh6De/NfLVvefQWz/Uj5O/oj9WPkAZij/+48m+gPb1Pcr3tT/gaMY/yT0PQAKLPT+My0k/yFEwvq3yDj9dEsQ/bKElv7l7GL8uEHK/JSR9P5HPEcAHamdAFC9wQLIgy74DEPC8axsYQGyPgr4p4hdAmcQ6P2pWY0DMg/S/6ox1QMDB4b44EtI/GAcsv/gFoj9erSm74Fb3v1D+o7/MBB4/mi7vP7qcK8ATlM2/oHXGvr56IMBBiVnASwC1vkoqnb/64R5AHRmgPwqN079eKDm/7zCxPo1W2z4HPxvAOBHtP8W7/T/tj4u/AFA5QG6VBMCw7xo/VVsnwEKIoT8o9jFAc7S4P6KhEUDfTvY/JMeov5D3AUCwXzrAQVsHP58HiL6Ks8u/sD5zv+tRKcAQERW/cIlkv61ntL4K6kPATiAYPI09E0B2nP6/b0mRvvLcqj68HF0/7ULMvll2zb+XO0Y/cjNLQLfSE0CjINq/o03QPyGeAz/bKGq/KkgAwNYK5r4kgaC/L/INQJ+K57wQPB4/lFkdv3umgr9CkRNAB4Oyv2STuj55yB4/2lyaP/7GxD+lzu0+cLPkvNjotj5zQnq/yRgjQIa8uT6ZRKu/o53QP5nSpb/9wiY/ksv5P8K+SEBp47k/q9IGv2XOIkA=",
          "dtype": "f4"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "blue",
          "size": 6
         },
         "mode": "markers+text",
         "showlegend": false,
         "text": [
          "eugene",
          "go",
          "telos",
          "insurrectionary",
          "welfare",
          "interviews",
          "dutch",
          "greenspan",
          "mentally",
          "larmes",
          "although",
          "camel",
          "iade",
          "showed",
          "depended",
          "overhead",
          "surmounted",
          "shyness",
          "reel",
          "promotional",
          "pretending",
          "intentionally",
          "surrender",
          "hector",
          "sexuality",
          "visions",
          "cbs",
          "deficit",
          "new",
          "loday",
          "weapon",
          "morte",
          "spite",
          "identifies",
          "organising",
          "dilorenzo",
          "burney",
          "archaic",
          "used",
          "profit",
          "engineer",
          "rainbow",
          "noticing",
          "clear",
          "rivers",
          "campaigning",
          "altruisme",
          "les",
          "exceeding",
          "tackle",
          "tzetzes",
          "broken",
          "games",
          "odyssey",
          "major",
          "virtuously",
          "introduction",
          "ramifications",
          "limp",
          "school",
          "harry",
          "initiate",
          "cheated",
          "approved",
          "parliamentary",
          "performances",
          "pool",
          "prefers",
          "discharge",
          "residents",
          "homer",
          "kneels",
          "county",
          "dares",
          "com",
          "shares",
          "climatic",
          "experiences",
          "disguises",
          "ideologies",
          "storytellers",
          "ibycus",
          "retardation",
          "catechism",
          "heatstroke",
          "facets",
          "tiepolo",
          "categoriae",
          "valued",
          "economical",
          "net",
          "manipulation",
          "kidnap",
          "calm",
          "beach",
          "respiratione",
          "wade",
          "writers",
          "spooner",
          "bill",
          "farm",
          "oases",
          "behaved",
          "road",
          "cellular",
          "barbershop",
          "espouses",
          "eastern",
          "un",
          "question",
          "receive",
          "means",
          "interested",
          "marxism",
          "righteous",
          "seemed",
          "seward",
          "prosperous",
          "creating",
          "divine",
          "misdirection",
          "exposed",
          "disorders",
          "amendment",
          "office",
          "feminist",
          "unrealistic",
          "william",
          "icd",
          "epicureans",
          "buddhism",
          "worship",
          "reasoned",
          "prepare",
          "expansion",
          "leventhal",
          "fowlers",
          "nationalists",
          "commercial",
          "computed",
          "participating",
          "emancipation",
          "rutgers",
          "understand",
          "das",
          "appomattox",
          "assist",
          "knows",
          "kelly",
          "healing",
          "kearns",
          "primitivism",
          "class",
          "arthur",
          "terrific",
          "http",
          "abroad",
          "multiple",
          "wake",
          "largest",
          "slanderers",
          "panicked",
          "ranges",
          "soil",
          "nursery",
          "servitude",
          "accounted",
          "back",
          "talk",
          "tremendous",
          "mid",
          "filter",
          "irving",
          "study",
          "applications",
          "planets",
          "tijuana",
          "divisions",
          "lix",
          "eclectic",
          "lash",
          "comprise",
          "wearing",
          "investigated",
          "washington",
          "principle",
          "colonies",
          "provisional",
          "films",
          "apart",
          "refuge",
          "roots",
          "flag",
          "caelo",
          "military",
          "energies",
          "ascended",
          "warms",
          "states",
          "dans"
         ],
         "textfont": {
          "size": 9
         },
         "textposition": "top center",
         "type": "scatter",
         "x": {
          "bdata": "/YigP/Wj7L9t9ARAyDGnv1Em5T/rCo+/uFxfvbkv7b7/XB7AoEoawHORKb/dQMe+ryL2vyt9bcAXTUi+upqEP+l+KcBa8Jm+QJ+2vuOPoT+OVr8/khfmvypnpD7Hle+/ABixvw2HR8CSgi/AdNeRPk4TqT9vqCm/oILwvy43AUA6+NI/JzfyP6u9d8DI3hA/N1lJQJLRmT9/tCC/npghP/XnEz/+KM49eXG3voi1Aj/cnxFAnd4ZwBFDEECzen8/XElGwMVHTsCQUwvAakEvwGJk9T/1AXu/bB2KwBBSLsAR61pA7zAJwJOf+b/p+htA5GKFv7tiw79toS69wsgJPw21u7/ud+i9AKIeQPO23z7iaAbA6iBIv189QD8M7ynAekdYvxIv37wy/BY/ejCovxiF6T7EHSNAFGIwwOT2IT/n90jABoqnv1ckkL8Chpe/r21UvizxW7+8cRe/a1knQNlAksC3qiDAtxmQwDFkVr89fgu/nyVOv5I5vz/owyFAlrPEv9aVmL58Mbe/jaJjwEuVm8C6662/Q9AvPz0dMr+HYGDA38kLQMUrJcBEExE9o5jAv6AX5T/rd1bA9WSrvnouDcA6ykxAoSjUP/Fdtr4pk0M/YPUmwAxQCUDmKfk/U1e+v2NJZr+w1o3A+Q4RwKT6o7+nQio/awshwMlEZMD5pTLAfuJwvnWq4zzWmOO/LqArwGrjZj/VDrA/fSoovxcw/L/wi2BAM7oZP6s9hMDf2ri+5dInwMfcDsCHPbE+Ren1v+trbsAq3H8/jVlBvUyWKkCTYQG+mvOovnNiDb/TXvI/E2vQv9t5VMArpb6/bvHHP34hYr6JWci/bplrP2XPDUBI28O/W55+P6sWIMBXp9S/0vWhvqioSsCAxd4+ZX+LP6ffvD+6ZyrArWeiPqfyST+Wv1M/K9y+v77/LsCGMEg/3DsVv3Crzr9Yrns/iu9Kv+ar3T9XlMq/k+TCPpYIAz3ruFy/Di9xwAYk3z1flb6/l1FfwFTSlb3P98W/nlpVv5GsEj+GZQrAkCeUv58Sv79vtltAAKSbwP7dWL8=",
          "dtype": "f4"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "VRy0PfGt7D9Kx9u/1O3bPqPnUcCIa8A+AsMRPpR/7b+L2RDA0plIwHXyDcCwg/q/ga+FwBmBpb8rD709OE6ivkHSrT9/hbo/gMMFQIYHNb89lApAhfCPvWajRT/86Jk/tcwWwMLTBkBbFb6+kmWbv1wciMCSyPa+8pEcQKf71b5Rp0fAX60fwNWEgj+pMS4/V7ksv3GI7D2MDk7AlI2ov6OCb8CBh4s/6ZwbPzsjQ0Axt9I/eagoQGQutL8AQbK/MdgWwNAslL84nwE/P9czPxZejb6B5Qs/vNoJvR2oVsD6O74/iwJcv31bsj81Dbi/sLTwvQ5XGsAozNC/pApIwP1eDr8R68g/64PyPZRCkz8PacM/UMGHwPWmWUCE4za+fKMPQFKw/T+DG6c/zS4JQOvIJcBNF+y/TEe3P9cepj8t7a6/aS9nP4Rfp78iLIC/9jXxv0ZlkL9f8C3AutPPPgZrfL93rB5AQUR2viPArr9o/rE/hqmqP3QNnD9YSXs/D6+RvtWjicDdntK/Fm6Kv9y4pL9JqUG/KAAywFhOMT9vSdo/XRbFvkuEEsACJfm+gvv0PonnNsDZFf8+TCEzQGgnWMAJ4iG/9ny1P6IzK7/Ba6Y9UjtAQDQ8Rz83uME/rtGbP7R/jL74sOI//Vmgv7PTgT6SslHANWh/P42opz8Tcks+3PKVPup3tD79wj1Ahkm+P94umz/ofP4+1bg+QEbco76zSQk/F7IoP0kdVr6LLYfAG41Evf/YFD/Gwja/9lhAPSjTHMDX7No/U3J8QBPuE0C8q4K/9dySP7CRSkBpRitAyFTVv0Kcgb/71GTAC9MzPx7gwL5B5Qm/Ev9nv5n3/L6Sm17A6roRvkjHB8Artsa/mvAlQJH7Hj8MygHAwmalv3rYub8nA02/LIrjvhVms7+8bkdAUhycv3EXwD7H3NM/7EFMQOD/Gr8Z/RBAdHaLv/v8Er/89UpAIqYfPye/u7+rZus/eOsoQNUhkD7Y5m9A47sxwCqtPMDFujrAWvDjvl7IPUAjQwjAmPuOP/MEgr+Prfa/fEP/PkAilj0=",
          "dtype": "f4"
         },
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Skip-Gram",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "CBOW",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 20
         },
         "text": "t-SNE Projection - CBOW vs Skip-Gram",
         "x": 0.5,
         "xanchor": "center"
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tsne_plot(embedding_list:list, labels, model_names:list, title='t-SNE Projection - CBOW vs Skip-Gram'):\n",
    "    # Create subplots for visualization\n",
    "    fig = sp.make_subplots(rows=1, cols=2, subplot_titles=model_names)\n",
    "\n",
    "    for idx, (embeddings, name) in enumerate(zip(embedding_list, model_names), start=1):\n",
    "        # Initialize UMAP object\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        # Reduce dimensionality\n",
    "        proj = tsne.fit_transform(embeddings)\n",
    "\n",
    "        # Create dataframe with coordinates for each data point and their corresponding words\n",
    "        df = pd.DataFrame({\n",
    "            'x': proj[:, 0],\n",
    "            'y': proj[:, 1],\n",
    "            'word': labels\n",
    "        })\n",
    "\n",
    "        # Create scatter plot with text annotations\n",
    "        scatter = go.Scatter(\n",
    "            x=df['x'], y=df['y'],               # Data\n",
    "            mode='markers+text',                # Show points and labels\n",
    "            text=df['word'],                    # Words to be annotated\n",
    "            textposition='top center', textfont=dict(size=9),         # Position and font size of the labels (words)\n",
    "            hoverinfo='text',                   # Hovering shows the word\n",
    "            showlegend=False,\n",
    "            marker=dict(size=6, color='blue')\n",
    "        )\n",
    "        # Append the scatter plot to the subplot\n",
    "        fig.add_trace(scatter, row=1, col=idx)\n",
    "\n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title=dict(text=title, x=0.5, xanchor='center', font=dict(size=20)),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "tsne_plot([skipgram_subset, cbow_subset], vocab_subset, ['Skip-Gram', 'CBOW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ac304",
   "metadata": {},
   "source": [
    "In this example, the same 200 word sub-sample was visualized using t-SNE for both our CBOW and Skipgram embeddings. Words closer together are more similar in meaning. Below are some specific examples which can be seen in the graphs above, with an example of how these might be used together on a Wikipedia article\n",
    "\n",
    "#### CBOW\n",
    "- \"accounted\" - \"heatstroke\" (heatstroke accounted for 10% of deaths)\n",
    "- \"experiences\" - \"divine\" (self-explanatory)\n",
    "- \"washington\" - \"farm\" (George Washington grew up in a farm in Vermont)\n",
    "- \"application\" - \"approved\" (also self-explanatory)\n",
    "- \"profit\" - \"welfare\" (profit and welfare are both words used in macroeconomics discussions)\n",
    "\n",
    "#### Skipgram\n",
    "- \"eugene\" - \"county\" - \"residents\" (Eugene, a city with 176,000 residents, is the county seat of Lane County, Oregon)\n",
    "- \"http\" - \"net\" (self-explanatory)\n",
    "- \"flag\" - \"filter\" (these two words can be used interchangeably when talking about something like a system which has flags or filters for bad words, for example)\n",
    "- \"buddhism\" - \"worship\" (also self-explanatory)\n",
    "- \"exceeding\" - \"commercial\" (the commercial activity in the area was exceeding its resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976d830",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **2. UMAP** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b329d6",
   "metadata": {},
   "source": [
    "UMAP is another algorithm used for dimension reduction based on manifold learning techniques and ideas from topological data analysis. It's a non-linear dimension reduction algorithm that seeks to `learn the structure of the data and find a low dimensional embedding that preserves the essential topological structure` of that manifold (lower-dimensional curved surface that resembles and preserves the characteristics of data stored in a high-dimensional space).\n",
    "\n",
    "UMAP has 4 major hyperparameters:\n",
    "- n_neighbors\n",
    "    - Controls how UMAP balances local vs global structure in the data, by constraining the size of the local neighbourhood UMAP will look at when attempting to learn the manifold structure of the data. **Low values focus on local structures, large values focus on the bigger picture.**\n",
    "- min_dist\n",
    "    - Controls how tightly UMAP is allowed to pack points together. **Low values result in clumpier embeddings, large values prevent stacking and result in the preservation of the broader topological structure.** \n",
    "- n_components\n",
    "    - Dimensions of the final reduction.\n",
    "- metric\n",
    "    - Controls **how distance is computed**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_plot(embedding_list:list, labels, model_names:list, title='UMAP Projection - CBOW vs Skip-Gram'):\n",
    "    # Create subplots for visualization\n",
    "    fig = sp.make_subplots(rows=1, cols=2, subplot_titles=model_names)\n",
    "\n",
    "    for idx, (embeddings, name) in enumerate(zip(embedding_list, model_names), start=1):\n",
    "        # Initialize UMAP object\n",
    "        umap_2d = UMAP(n_components=2, n_neighbors=2, min_dist=0.3, init='random', random_state=42, n_jobs=1)\n",
    "        # Reduce dimensionality\n",
    "        proj = umap_2d.fit_transform(embeddings)\n",
    "\n",
    "        # Create dataframe with coordinates for each data point and their corresponding words\n",
    "        df = pd.DataFrame({\n",
    "            'x': proj[:, 0],\n",
    "            'y': proj[:, 1],\n",
    "            'word': labels\n",
    "        })\n",
    "\n",
    "        # Create scatter plot with text annotations\n",
    "        scatter = go.Scatter(\n",
    "            x=df['x'], y=df['y'],               # Data\n",
    "            mode='markers+text',                # Show points and labels\n",
    "            text=df['word'],                    # Words to be annotated\n",
    "            textposition='top center', textfont=dict(size=9),         # Position and font size of the labels (words)\n",
    "            hoverinfo='text',                   # Hovering shows the word\n",
    "            showlegend=False,\n",
    "            marker=dict(size=6, color='blue')\n",
    "        )\n",
    "        # Append the scatter plot to the subplot\n",
    "        fig.add_trace(scatter, row=1, col=idx)\n",
    "\n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        title=dict(text=title, x=0.5, xanchor='center', font=dict(size=20)),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12810fda",
   "metadata": {},
   "source": [
    "For the UMAP dimensionality reduction, we chose the following hyperparameters:\n",
    "- **n_neighbors:** 3\n",
    "- **min_dist:** 0.1\n",
    "\n",
    "Since we want to observe the specific topological structure of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bea77d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": "blue",
          "size": 6
         },
         "mode": "markers+text",
         "showlegend": false,
         "text": [
          "eugene",
          "go",
          "telos",
          "insurrectionary",
          "welfare",
          "interviews",
          "dutch",
          "greenspan",
          "mentally",
          "larmes",
          "although",
          "camel",
          "iade",
          "showed",
          "depended",
          "overhead",
          "surmounted",
          "shyness",
          "reel",
          "promotional",
          "pretending",
          "intentionally",
          "surrender",
          "hector",
          "sexuality",
          "visions",
          "cbs",
          "deficit",
          "new",
          "loday",
          "weapon",
          "morte",
          "spite",
          "identifies",
          "organising",
          "dilorenzo",
          "burney",
          "archaic",
          "used",
          "profit",
          "engineer",
          "rainbow",
          "noticing",
          "clear",
          "rivers",
          "campaigning",
          "altruisme",
          "les",
          "exceeding",
          "tackle",
          "tzetzes",
          "broken",
          "games",
          "odyssey",
          "major",
          "virtuously",
          "introduction",
          "ramifications",
          "limp",
          "school",
          "harry",
          "initiate",
          "cheated",
          "approved",
          "parliamentary",
          "performances",
          "pool",
          "prefers",
          "discharge",
          "residents",
          "homer",
          "kneels",
          "county",
          "dares",
          "com",
          "shares",
          "climatic",
          "experiences",
          "disguises",
          "ideologies",
          "storytellers",
          "ibycus",
          "retardation",
          "catechism",
          "heatstroke",
          "facets",
          "tiepolo",
          "categoriae",
          "valued",
          "economical",
          "net",
          "manipulation",
          "kidnap",
          "calm",
          "beach",
          "respiratione",
          "wade",
          "writers",
          "spooner",
          "bill",
          "farm",
          "oases",
          "behaved",
          "road",
          "cellular",
          "barbershop",
          "espouses",
          "eastern",
          "un",
          "question",
          "receive",
          "means",
          "interested",
          "marxism",
          "righteous",
          "seemed",
          "seward",
          "prosperous",
          "creating",
          "divine",
          "misdirection",
          "exposed",
          "disorders",
          "amendment",
          "office",
          "feminist",
          "unrealistic",
          "william",
          "icd",
          "epicureans",
          "buddhism",
          "worship",
          "reasoned",
          "prepare",
          "expansion",
          "leventhal",
          "fowlers",
          "nationalists",
          "commercial",
          "computed",
          "participating",
          "emancipation",
          "rutgers",
          "understand",
          "das",
          "appomattox",
          "assist",
          "knows",
          "kelly",
          "healing",
          "kearns",
          "primitivism",
          "class",
          "arthur",
          "terrific",
          "http",
          "abroad",
          "multiple",
          "wake",
          "largest",
          "slanderers",
          "panicked",
          "ranges",
          "soil",
          "nursery",
          "servitude",
          "accounted",
          "back",
          "talk",
          "tremendous",
          "mid",
          "filter",
          "irving",
          "study",
          "applications",
          "planets",
          "tijuana",
          "divisions",
          "lix",
          "eclectic",
          "lash",
          "comprise",
          "wearing",
          "investigated",
          "washington",
          "principle",
          "colonies",
          "provisional",
          "films",
          "apart",
          "refuge",
          "roots",
          "flag",
          "caelo",
          "military",
          "energies",
          "ascended",
          "warms",
          "states",
          "dans"
         ],
         "textfont": {
          "size": 9
         },
         "textposition": "top center",
         "type": "scatter",
         "x": {
          "bdata": "YR/hQIo92UDGbbdA6TuBQMqd8EAvtLtAZKtvQM2BCUEbiINA69rbQAbq3UB3AQhBtkeWQBOGqkCx6tlA5BfgQNnhtEA/BF1AEEnSQH4V+0BfHgZBHwvcQKOq3kARQFFAEdGyQO/LvEDeO9dAFKC/QKwaAkEAvtdAk2KxQHYBnUC65YFA+LLrQO59DUHMmvRAIZ2mQBkH1kAN8gxBdfffQBLf6kAYFuJAqavVQJzUC0FGBoNAp4N6QH0gz0BCqPpAj/vJQG6noUAoQZVAptLHQJndskAjM/lAxg3EQPkE6UAk9V9AUGTGQDKqBEGBBPdAHPCPQNEum0ADPupARY4EQeusoUBlwcFAP3rrQMBrkUCo3P5A9oSYQKgXxkBaKjFAKhT+QP4dlEBgOaVA2FydQNIzmkDrD6JAq2mmQDpUu0DDcBJBZ2u7QNHspEBuXapAoEqOQOwqEEHOVd5A+UG8QMLeukB7YihA/GLXQN0cj0BKVZZADyHaQLqLl0BJC2lAYsIOQXxg40Cgir1AGOhuQIoQCUF0dPNAPJpLQHHGkUDx8eZAWZMAQTnj6EDCpYNAMp0zQD6eiUBD5wZBqUK1QPCl8kCqT4hAM07gQMpCw0Dy9r9AM/60QLroD0E4GLpAviWrQHGow0A/xMdA/SvnQFcYvEDZBZtA4MbMQAUKhkBjqqZA03T2QE3HxUBroIhAbZL8QH7UnUDvE5JAPocmQPVz10Cm+VpAlwC0QDgtm0BONZxAMQ/PQAAM9UCUzoRAshsIQf7xBUHTd8NAAfqiQF7rlkBpjqNAY/YJQVGcqUDlMs9AvHnYQHGbsUAlQLtAlw43QKw0hkBq9HVAyo7uQFvl00BuJLdAJ+ToQKyuN0COvdVAP23CQN4ex0B7Md5Ao5LrQOe460DBWGlA70fNQCcwzUDAx51AXYfTQOXDDkH7vLJAck/AQLLFzkBuyOJAweyjQPlCwUB/HeRAP9njQLU62UCTV5FAItIHQdXaxUByirZAivbgQOtQEUHfqcVAuwAMQTLG+0BlWttAvHYOQX/k7kBvg7ZAOo76QHckkkA=",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "U7wAQC1ntUAbzBZAorWiQKaNekA/f41A/pHsQFj4jkB1d/JAFYYBQfIq40A/i11A2NTjQLqpi0CdMTFAaAzhQPijPkAZYLtA4LyEQEq9UECLm7NAtZPrQGDRsEABFLBAjb73QLet0EB0Aa9AfhwjQBq7ukCLydRAbC8ZQOCA40CVgNVAzecLQGbaMUAzPZNAVQysQL7yA0BRE1tAwrjkQOLvhECtwZdABx/EQJCibkDoJu9A42EKQTCcvEB+CNJAH5a+QHRsz0Bm3Q5BB6BAQEK540Aa+/FA66s4QHVXAkEkNKZAC9PrQMhFTkAxL4pAjkNYQCK4kkDt8oVAgIu7QAGVj0Cwq5FA/+p+QFpPq0BQHstAAwYOQcETk0BK+qdAebjRQMwQqUAw7+VAywA8QG9M7UCOazBA9HBCQGtLjECvvJtAZxMbQJBbZkCCSLxAqn2ZQGr+kUA6s+xAtWn2QHKzsEAQJK5AW8fYQG/t+kCKsA5BxYgEQS9JRkAqCelAuIx+QJMhxED0SK5ASefrQMPYdEAwvLVAiluuQDuq/UDvqMVAvWHOQONaxUCidp9AsiOwQKhek0CDI7ZA/kwNQUvLZUBDzwdBDJilQHQkdkBBqCxAGQ8OQabjcUBInahADdMKQd3w0kCNqshAnF5hQHqkDUEVCj1AgI52QCwGB0HmMQhBNVBIQNG2IUB1fJRAKpRBQCmfPEDvZ1JAxD2sQNsWwkDCMr1A1z0PQd756EBCb5FAd7F0QMjMmEDAM9JAhnmZQNTEsEBCy8NADOvWQIjX5ECpRFtAQKUyQFn96EBsJQxAPmN0QONzskDWO+RABEavQM6wy0CbtgpBE0USQAIaEEDiuIVAgna/QA5joEB67/s/loXuQM+Fx0BVObNA5zACQeE+C0BwRqJAAR66QE0G4kD37dJAS8/wQEDTp0DRiYJAdwiUQJIXUEDfJiFA19UFQfLUskAQcc9A+/tnQMrsFUBZA6tAlLhvQEOAbkAjT/9ASmCAQGN3n0C+f25AL4A4QILi6kAC0e5A9+AtQEBSikCnZuFAAvI+QOYjDUE=",
          "dtype": "f4"
         },
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "blue",
          "size": 6
         },
         "mode": "markers+text",
         "showlegend": false,
         "text": [
          "eugene",
          "go",
          "telos",
          "insurrectionary",
          "welfare",
          "interviews",
          "dutch",
          "greenspan",
          "mentally",
          "larmes",
          "although",
          "camel",
          "iade",
          "showed",
          "depended",
          "overhead",
          "surmounted",
          "shyness",
          "reel",
          "promotional",
          "pretending",
          "intentionally",
          "surrender",
          "hector",
          "sexuality",
          "visions",
          "cbs",
          "deficit",
          "new",
          "loday",
          "weapon",
          "morte",
          "spite",
          "identifies",
          "organising",
          "dilorenzo",
          "burney",
          "archaic",
          "used",
          "profit",
          "engineer",
          "rainbow",
          "noticing",
          "clear",
          "rivers",
          "campaigning",
          "altruisme",
          "les",
          "exceeding",
          "tackle",
          "tzetzes",
          "broken",
          "games",
          "odyssey",
          "major",
          "virtuously",
          "introduction",
          "ramifications",
          "limp",
          "school",
          "harry",
          "initiate",
          "cheated",
          "approved",
          "parliamentary",
          "performances",
          "pool",
          "prefers",
          "discharge",
          "residents",
          "homer",
          "kneels",
          "county",
          "dares",
          "com",
          "shares",
          "climatic",
          "experiences",
          "disguises",
          "ideologies",
          "storytellers",
          "ibycus",
          "retardation",
          "catechism",
          "heatstroke",
          "facets",
          "tiepolo",
          "categoriae",
          "valued",
          "economical",
          "net",
          "manipulation",
          "kidnap",
          "calm",
          "beach",
          "respiratione",
          "wade",
          "writers",
          "spooner",
          "bill",
          "farm",
          "oases",
          "behaved",
          "road",
          "cellular",
          "barbershop",
          "espouses",
          "eastern",
          "un",
          "question",
          "receive",
          "means",
          "interested",
          "marxism",
          "righteous",
          "seemed",
          "seward",
          "prosperous",
          "creating",
          "divine",
          "misdirection",
          "exposed",
          "disorders",
          "amendment",
          "office",
          "feminist",
          "unrealistic",
          "william",
          "icd",
          "epicureans",
          "buddhism",
          "worship",
          "reasoned",
          "prepare",
          "expansion",
          "leventhal",
          "fowlers",
          "nationalists",
          "commercial",
          "computed",
          "participating",
          "emancipation",
          "rutgers",
          "understand",
          "das",
          "appomattox",
          "assist",
          "knows",
          "kelly",
          "healing",
          "kearns",
          "primitivism",
          "class",
          "arthur",
          "terrific",
          "http",
          "abroad",
          "multiple",
          "wake",
          "largest",
          "slanderers",
          "panicked",
          "ranges",
          "soil",
          "nursery",
          "servitude",
          "accounted",
          "back",
          "talk",
          "tremendous",
          "mid",
          "filter",
          "irving",
          "study",
          "applications",
          "planets",
          "tijuana",
          "divisions",
          "lix",
          "eclectic",
          "lash",
          "comprise",
          "wearing",
          "investigated",
          "washington",
          "principle",
          "colonies",
          "provisional",
          "films",
          "apart",
          "refuge",
          "roots",
          "flag",
          "caelo",
          "military",
          "energies",
          "ascended",
          "warms",
          "states",
          "dans"
         ],
         "textfont": {
          "size": 9
         },
         "textposition": "top center",
         "type": "scatter",
         "x": {
          "bdata": "If21QJCzFUDm6x1AvToeQHhTv0BdTbFAhg/LQKUge0CMK8Y/a95sQJ/DVEBN1y5AtnEuQIkyhED03YZAn3RaQLBhuUBBzipAxWRPQKzIXEBLbgJAmwtLQMnTbz90zwtAY7s5QJQjnEBVTAdAn4x0QFbFnkAbE6RAHTgtQDfOxEBZ8sNAgHWQQDlXzkCHoCNAZvtdQEgp30B1v/lAs/GEQFb0uEDh+sJAyRyEQBU2XUCVahdApSshQDISkECrMWRA3GmJQEGbtUAl6rRAm+nJQN/OTkAmejBA41KmP6O3KkCDjTxAx8iHQHRzuj8a+q5AFRvWQAIx6D9NmphA9ECrPwu+4kDgRlFAeh/jQKa+BUGb/IhACuvLQIWt50DTl6FAY0KmQEBn1UBOzrs/3qyGQClWq0DGUqZAxwtRQKmL1UCp3aBA7ApLQK6XyEBwoGNASzocQBr2yUBAgKhAjLu7QKmB9ECay/9AQlKFQA6t50CVE7A/VYycQMYEmkDPvsc/sxexQDQ4b0Cuw45A7JOXQLGsOUBuSARBvndjQOSaWkDU4ttA+ZqnQFL4qEBnINw/xY2yQPog9T/w4YVARKTHP/UdiUC5xBxA/qWtQHOAX0CAegVBfqxDQCmQ2UA9XqVAEd9iQDFa0UCepdxAKYXKQPr4v0ApzVlA7SaWQCQP3kB66c9AMmO8P8b7/kDzh41A0h9XQIbDi0BboxFAtV2BQJMRXEDWNi5AP8jUQJ7AoUBVR5dAef4FQQrys0DvoAtAlSWoQPddikDWNrdAPWJ9QCM5WUDL/eQ/3yXOP22ykkAC/6dAcHpyQEXcUUDN7D5AFDwYQLtkBEDydMlAsamHQISF0kDHzhpAm2FlQAHwtkCtYMI/yY2kQKwU0UDYP9Q/MMbAP1HP1j/05atA+5XEQIWkmEBA8fk/BEJKQOeneUBXbu9A+PmTP04rg0ASXV9AMcllP1HxxEAb1GNAdbnoQKd5QUBR6OlAkLKJQDhj1UBQ1CBAohDaQL08MEBC75tAzajFQICip0A93otAZVccQDfBwUD1eZFA/1ymQHUuoEA=",
          "dtype": "f4"
         },
         "xaxis": "x2",
         "y": {
          "bdata": "jJqsPyENbEAnlkdAxgyzP9D0UECMRTa/vogwvpeqBkB7dz0+DFAVP6DPrz/Dwc8/zGEHQP/wJ0ABTxU/aRu2vvgR9j7nHQQ/z/xUQB6knD+XRnJARHfZP+vl+D9q50G/r6eGv29Bmb8KV3pAGlYLQL035L4AcgtAuFRAvwxRfD/Y07m+U5/lvZcfT7+h3llASE/dv3Sl7z+Z7II/IpSMQEKMab9gtHw/7+W5v/z4KkASdgVA/fjMP/kbXECErl1Ay4rQP9ZuxT5A+zw/HBbEvhZxO0D98lFANCaxP9HWX0A9AI+/9XU6Pk9gs70FHLS+ShwyQMueK0A32SdA3ptvvhj99T/UZtY+NlHWP+widT+r6l0/a/Q6QDQ/5z//C20/mswpQAYWKUDaUwZASoONQDXuAEBdp/Q/7AGSv05PSUB2Xbs/FBabPpcCPj/vz0M/KdJTQMHU474b1QO/lrMJvyqdMD/Sb4E/B88PQJ5iB0DQ9qc/GbGsvw0CtD8PqJI/uADiP/jXtD8EAEhATQDgPu3KJkBZUXY/ptkOP16R1r8THS1AUcKAv0FymD8UW+s+kLkZQBFC9T/BB1tAANO0P/GWj0DdrklARolFvzOd9D6qRYI/7bc8QMM5LECggX0/YqhkQIQ8O78YO9E/ZhrAvrTJS0ArA6E/aRmbPhWmpr4Dqoa+ccuXvr0AnD9Aj2BA9cdkQOtHgr6kp/Y/KpfAPnBpGUDoaANA/agevopsqr/fdUBAAzxMP/KIML/Srvk/lX08QHp5BEDjLR5AxvsKQA8gEz/cK/I/eBfUvnZ+oL0M/x1AceOuvzbngr9W7kFAMN1BQIk6gEBN6VU/mlgeQDhXN0AdY4M/LOEfQIaVV78M3s6+5FcpQDGePEDmtwQ/aQD2P4jE9z5y3xJAyCNpvnSZD73Pbia/Qm0MPkPzNEAjU+w//1vEP0kMi0CHRte+Q9gEQO0IQ0Dt/q8/g/odQM/3Tb2FdxZAqY6BvgAGXL/4h+k/SnG7vhNI7L6HQzNAYC0zQAswnT9PweI/t5uFPy7YGL//mbu/MG89QJWElj8=",
          "dtype": "f4"
         },
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Skip-Gram",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "CBOW",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 20
         },
         "text": "UMAP Projection - CBOW vs Skip-Gram",
         "x": 0.5,
         "xanchor": "center"
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "umap_plot([skipgram_subset, cbow_subset], vocab_subset, ['Skip-Gram', 'CBOW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c1afd",
   "metadata": {},
   "source": [
    "#### Skipgram\n",
    "- \"dutch\" - \"bill\" (probably talking about an influential bill proposed in the Netherlands)\n",
    "- \"colonies\" - \"farm\" (a little bit self-explanatory)\n",
    "- \"servitude\" - \"ramifications\" (probably referring to the implications servitude has)\n",
    "- \"initiates\" - \"participating\" - \"parliamentary\" (words that may be commonly found together)\n",
    "- \"comercial\" - \"prosperous\" (probably talking about a prosperous commerce)\n",
    "\n",
    "#### CBOW\n",
    "- \"emancipation\" - \"buddhism\" - \"valued\" (probably talking about details of the buddhist religion)\n",
    "- \"manipulation\" - \"archaich\" - \"parlamentary\" (may refer to a critic to the government systems)\n",
    "- \"office\" - \"wellfare\" (relation may be built upon the uses of the office)\n",
    "- \"shares\" - \"profit\" (self-explanatory)\n",
    "- \"feminist\" - \"writers\" - \"promotional\" (may refer to the rise of feminism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac614a2",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Conclusions** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f34a0",
   "metadata": {},
   "source": [
    "CBOW and Skipgram are very useful Word2Vec approaches to draw word meaning and similarity in a way that a computer can understand. For this specific example of Wikipedia articles, CBOW seems to be a much better approach for building our word embeddings. The final training loss was about 8 times lower with CBOW compared to Skipgram. In our testing, the groupings of words obtained from CBOW also make a lot more sense, and Skipgram missed some very obvious ones like \"http\"-\"net\" and \"shares\"-\"profit\". \n",
    "\n",
    "Even though Skipgram was less useful for this specific task, it could be more useful in others. For example, Skipgram would be more ad hoc for a system that can predict the next word you are gonna type, a useful feature found in many smartphones. This is due to Skipgram's fundamental nature as well as its capacity to work better with corpora featuring vocabulary that doesn't repeat much, which wasn't the case for our Wikipedia sample.\n",
    "\n",
    "This project was also a great exploration of dimensionality reduction, particularly with UMAP. We had already used t-SNE before but UMAP was a new tool for us, and it actually adjusted a lot better to this task than t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d3f54",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d> **Bibliography** </font>\n",
    "\n",
    "- Riva, M. (2025, February 13). _Word Embeddings: CBOW vs Skip-Gram_. Baeldung CS. https://www.baeldung.com/cs/word-embeddings-cbow-vs-skip-gram\n",
    "- Van der Maaten, L., Hinton, G. (2008) *Visualizing Data using t-SNE*. **Journal of Machine Learning Research**. https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\n",
    "- McInnes, L. (2018). _Basic UMAP Parameters_. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "- Plotly. (n.d.). _t-SNE and UMAP projections in Python_. Plotly. https://plotly.com/python/t-sne-and-umap-projections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
